<!doctype html>
<html lang="en">
    <head>
        <title>Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future</title>
        
        <link rel="icon" href="assets_common/icons/worldbench-circle.png" type="image/x-icon">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Rubik&display=swap" rel="stylesheet">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <script src="./static/js/distill_template.v2.js"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>

        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
    </head>

    <body>
        <div class="header-wrapper">

            <div class="header-container", id="header-container">
                <div class="header-content">
                <h1>
                  Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future
                </h1>

                <h2 style="margin:2.2rem 0 1.8rem; display:flex; align-items:center; gap:0.4rem;">
                  <span class="icon is-small">
                    <img src="assets_common/icons/worldbench-circle.png" style="height:1.8em; vertical-align:text-top;">
                  </span>&nbsp;WorldBench Team
                </h2>


                <div class="button-container">
                    <a href="" class="button paper-link" target="_blank">
                        <span class="icon is-small">
                            <i class="ai ai-arxiv "  style="height: 1.5em;"></i>
                        </span><span>arXiv</span>
                    </a>
                    <a href="assets_common/papers/vla4ad.pdf" class="button paper-link" target="_blank">
                        <span class="icon is-small">
                            <img src="assets_common/icons/pdf.png" style="height: 1.4em;">
                        </span><span>PDF</span>
                    </a>
                    <a href="https://github.com/worldbench/awesome-vla-for-ad" class="button" target="_blank">
                        <span class="icon is-small">
                            <img src="assets_common/icons/github.png" style="height: 1.4em;">
                        </span><span>GitHub</span>
                    </a>
                    <a href="https://huggingface.co/spaces/worldbench/vla4ad" class="button" target="_blank">
                        <span class="icon is-small">
                            <img src="assets_common/icons/hf.png" style="height: 1.5em;">
                        </span><span>Leaderboard</span>
                    </a>
                </div>
                </div>
                <div class="header-image">
                    <img src="assets/vla4ad/figures/vla4ad.webp" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>




        <d-article>
            <d-contents>
                <nav>
                    <h4>Contents</h4>
                    <div><a href="#definition">Definition</a></div>
                    <div><a href="#taxonomy">Taxonomy</a></div>
                    <div><a href="#projects">Collections</a></div>
                    <div><a href="#contributors">Contributors</a></div>
                </nav>
            </d-contents>


            <section id="introduction">
              <div class="wb-section">
                  <div class="wb-kicker">Introduction</div>
                  <div class="wb-hr"></div>
                  <p class="wb-lead">
                    This survey reviews <strong>Vision-Language-Action (VLA) models</strong> â€” an emerging paradigm that integrates 
                    visual perception, natural language reasoning, and executable actions for autonomous driving. We trace the evolution 
                    from traditional <strong>Vision-Action (VA) approaches</strong> to <strong>modern VLA frameworks</strong>.
                  </p>

                  <figure style="width: 100%; max-width: 1000px; display: block; margin: 24px auto;">
                    <img src="assets/vla4ad/figures/teaser.png">
                  </figure>
                <br><br>
              </div>
            </section>










            <section id="definition">
              <h2>Definition</h2>
              <div class="wb-hr" style="margin-bottom:1.25rem;"></div>

              <style>
                #definition .wb-grid{
                  grid-template-columns: 1fr !important; max-width: 900px; margin: 0 auto; gap: 1rem 1.25rem;
                }
              </style>

              <div class="wb-grid">
                <!-- Vision-Action (VA) Models -->
                <article class="wb-card" id="va-models">
                  <h3>
                      <i class="fas fa-film" aria-hidden="true"></i>
                      <a class="wb-anchor" href="#va-models">Vision-Action (VA) Models</a>
                    </h3>
                    <p>
                      A vision-centric driving system that directly maps raw sensory 
                      observations to driving actions, thereby avoiding explicit modular decomposition 
                      into perception, prediction, and planning. VA models learn end-to-end policies through imitation learning 
                      or reinforcement learning.
                    </p>
                    <p class="wb-cap">
                      <strong>Typical inputs</strong>: Multi-View RGB Images, LiDAR Point Clouds, Ego Vehicle State, Maps.
                    </p>
                    <div class="wb-meta" aria-label="VA Models tags">
                      <span class="wb-chip">End-to-End Models</span>
                      <span class="wb-chip">World Models</span>
                      <span class="wb-chip">Imitation Learning</span>
                      <span class="wb-chip">Reinforcement Learning</span>
                      <span class="wb-chip">Trajectory Prediction</span>
                    </div>
                </article>

                <!-- Vision-Language-Action (VLA) Models -->
                <article class="wb-card" id="lidargen">
                  <h3>
                      <i class="fas fa-braille" aria-hidden="true"></i>
                      <a class="wb-anchor" href="#vla-models">Vision-Language-Action (VLA) Models</a>
                    </h3>
                    <p>
                      A multimodal reasoning system that couples visual perception with 
                      large VLMs to produce executable driving actions. VLAs integrate visual understanding, 
                      linguistic reasoning, and actionable outputs within a unified framework, enabling more interpretable, 
                      generalizable, and human-aligned driving policies through natural language instructions and chain-of-thought reasoning.
                    </p>
                    <p class="wb-cap">
                      <strong>Typical inputs</strong>: Multi-View RGB Images, LiDAR Point Clouds, BEV Features, Natural Language Instructions, 
                      Scene Descriptions, Traffic Rules, Ego Status, Context Demonstrations.
                    </p>
                    <div class="wb-meta" aria-label="VLA Models tags">
                      <span class="wb-chip">End-to-End VLA</span>
                      <span class="wb-chip">Dual-System VLA</span>
                      <span class="wb-chip">Chain-of-Thought</span>
                      <span class="wb-chip">Instruction Following</span>
                      <span class="wb-chip">Interpretable Reasoning</span>
                    </div>
                </article>
              </div>
              <br><br>
            </section>


            <style>
            #definition .wb-bridge {
              font-size: 1rem;
              line-height: 1.7;
              color: #374151;
              margin: 1rem 0 .75rem;
            }
            #definition .wb-grid-4 {
              display: grid;
              grid-template-columns: repeat(4, minmax(0, 1fr));
              gap: 1.25rem;
              margin-top: .5rem;
            }
            @media (max-width: 1200px) {
              #definition .wb-grid-4 { grid-template-columns: repeat(3, minmax(0, 1fr)); }
            }
            @media (max-width: 1024px) {
              #definition .wb-grid-4 { grid-template-columns: repeat(2, minmax(0, 1fr)); }
            }
            @media (max-width: 640px) {
              #definition .wb-grid-4 { grid-template-columns: 1fr; }
            }
            #definition .wb-card h4 {
              display: flex; align-items: center; gap: .5rem;
              font-size: 1rem; margin: 0 0 .35rem;
            }
            #definition .wb-card p.small {
              margin: .35rem 0; color: #4b5563; line-height: 1.6;
            }
          </style>





          <section id="projects">
              <h2>Collections</h2>
              <div class="wb-hr" style="margin-bottom:1.25rem;"></div>
          </section>
          <section id="va-papers" class="papers-section" data-section-key="va" data-initial="3">
              <h2>Vision-Action Models</h2>
              <div class="bar" aria-live="polite">
                <div class="tabs" role="tablist" aria-label="Vision-Action Model Categories">
                  <button class="tab-btn tab-1" role="tab" aria-selected="true" aria-controls="va-panel-a" id="va-tab-a">Action-Only Models</button>
                  <button class="tab-btn tab-2" role="tab" aria-selected="false" aria-controls="va-panel-b" id="va-tab-b">Perception-Action Models</button>
                  <button class="tab-btn tab-3" role="tab" aria-selected="false" aria-controls="va-panel-c" id="va-tab-c">Image-Bsed World Models</button>
                  <button class="tab-btn tab-4" role="tab" aria-selected="false" aria-controls="va-panel-d" id="va-tab-d">Occupancy World Models</button>
                  <button class="tab-btn tab-5" role="tab" aria-selected="false" aria-controls="va-panel-e" id="va-tab-e">Latent World Models</button>
                </div>
                <div class="search" title="">
                  ðŸ”Ž <input class="paper-search" type="search" placeholder="Search: Title / Author / Source" />
                </div>
                <span class="count">0 items</span>
              </div>
              
              <!-- Action-Only Models -->
              <section id="va-panel-a" class="panel" role="tabpanel" aria-labelledby="va-tab-a" aria-hidden="false">
                <div class="list"></div>
              </section>
              
              <!-- Perception-Action Models -->
              <section id="va-panel-b" class="panel" role="tabpanel" aria-labelledby="va-tab-b" aria-hidden="true">
                <div class="list"></div>
              </section>
              
              <!-- Image-Based World Models -->
              <section id="va-panel-c" class="panel" role="tabpanel" aria-labelledby="va-tab-c" aria-hidden="true">
                <div class="list"></div>
              </section>
              
              <!-- Occupancy World Models -->
              <section id="va-panel-d" class="panel" role="tabpanel" aria-labelledby="va-tab-d" aria-hidden="true">
                <div class="list"></div>
              </section>
              
              <!-- Latent World Models -->
              <section id="va-panel-e" class="panel" role="tabpanel" aria-labelledby="va-tab-e" aria-hidden="true">
                <div class="list"></div>
              </section>
            </section>


            <section id="projects">
                <hr>
            </section>


            <section id="vla-papers" class="papers-section" data-section-key="vla" data-initial="3">
              <h2>Vision-Language-Action Models</h2>
              <div class="bar" aria-live="polite">
                <div class="tabs" role="tablist" aria-label="Vision-Language-Action Model Categories">
                  <button class="tab-btn tab-1" role="tab" aria-selected="true" aria-controls="vla-panel-a" id="vla-tab-a">Textual Action Generator</button>
                  <button class="tab-btn tab-2" role="tab" aria-selected="false" aria-controls="vla-panel-b" id="vla-tab-b">Numerical Action Generator</button>
                  <button class="tab-btn tab-3" role="tab" aria-selected="false" aria-controls="vla-panel-c" id="vla-tab-c">Explicit Action Guidance</button>
                  <button class="tab-btn tab-4" role="tab" aria-selected="false" aria-controls="vla-panel-d" id="vla-tab-d">Implicit Representations Transfer</button>
                </div>
                <div class="search" title="è¾“å…¥å…³é”®è¯è¿‡æ»¤ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€å‡ºå¤„ï¼‰">
                  ðŸ”Ž <input class="paper-search" type="search" placeholder="Search: Title / Author / Source" />
                </div>
                <span class="count">0 items</span>
              </div>
              
              <!-- Textual Action Generator -->
              <section id="vla-panel-a" class="panel" role="tabpanel" aria-labelledby="vla-tab-a" aria-hidden="false">
                <div class="list"></div>
              </section>
              
              <!-- Numerical Action Generator -->
              <section id="vla-panel-b" class="panel" role="tabpanel" aria-labelledby="vla-tab-b" aria-hidden="true">
                <div class="list"></div>
              </section>
              
              <!-- Explicit Action Guidance -->
              <section id="vla-panel-c" class="panel" role="tabpanel" aria-labelledby="vla-tab-c" aria-hidden="true">
                <div class="list"></div>
              </section>
              
              <!-- Implicit Representations Transfer -->
              <section id="vla-panel-d" class="panel" role="tabpanel" aria-labelledby="vla-tab-d" aria-hidden="true">
                <div class="list"></div>
              </section>
            </section>


            <section id="projects">
                <hr>
            </section>


            <section id="dataset-papers" class="papers-section" data-section-key="dataset" data-initial="3">
              <h2>Datasets & Benchmarks</h2>
              <div class="bar" aria-live="polite">
                <div class="tabs" role="tablist" aria-label="Dataset Categories">
                  <button class="tab-btn tab-1" role="tab" aria-selected="true" aria-controls="dataset-panel-a" id="dataset-tab-a">Vision-Action Datasets</button>
                  <button class="tab-btn tab-2" role="tab" aria-selected="false" aria-controls="dataset-panel-b" id="dataset-tab-b">Vision-Language-Action Datasets</button>
                </div>
                <div class="search" title="è¾“å…¥å…³é”®è¯è¿‡æ»¤ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€å‡ºå¤„ï¼‰">
                  ðŸ”Ž <input class="paper-search" type="search" placeholder="Search: Title / Author / Source" />
                </div>
                <span class="count">0 items</span>
              </div>
              
              <!-- Vision-Action Datasets -->
              <section id="dataset-panel-a" class="panel" role="tabpanel" aria-labelledby="dataset-tab-a" aria-hidden="false">
                <div class="list"></div>
              </section>
              
              <!-- Vision-Language-Action Datasets -->
              <section id="dataset-panel-b" class="panel" role="tabpanel" aria-labelledby="dataset-tab-b" aria-hidden="true">
                <div class="list"></div>
              </section>
            </section>            

            <br><br>





            <section id="contributors">
              <div class="wb-section">
                
                <h2>Contributors</h2>

                <div class="contributors-list">
                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/tianshuai_hu.png"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Tianshuai Hu</div>
                      <div class="contributor-note">Core Contributor</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>  
                
                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/icons/worldbench-circle.png"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Xiaolu Liu</div>
                      <div class="contributor-note">Core Contributor</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>  

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/song_wang.jpeg"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Song Wang</div>
                      <div class="contributor-note">Core Contributor</div>
                    </div>
                    <div class="contributor-links">
                      <a href="https://songw-zju.github.io/" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="https://scholar.google.com/citations?user=Jj0jbL8AAAAJ&hl=en" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                      <a href="https://github.com/songw-zju" target="_blank" aria-label="GitHub"> <i class="fab fa-github"></i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/icons/worldbench-circle.png"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Yiyao Zhu</div>
                      <div class="contributor-note">Core Contributor</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>         

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/ao_liang.jpg"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Ao Liang</div>
                      <div class="contributor-note">Core Contributor</div>
                    </div>
                    <div class="contributor-links">
                      <a href="https://alanliangc.github.io/" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="https://scholar.google.com/citations?user=ocyBGGYAAAAJ&hl=en" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                      <a href="https://github.com/AlanLiangC" target="_blank" aria-label="GitHub"> <i class="fab fa-github"></i> </a>
                    </div>
                  </div>
                
                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/lingdong_kong.jpg" 
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Lingdong Kong</div>
                      <div class="contributor-note">Core Contributor, Project Lead</div>
                    </div>
                    <div class="contributor-links">
                      <a href="mailto:lingdong.kong@u.nus.edu" aria-label="Email"><i class="fas fa-envelope"></i> </a>
                      <a href="https://ldkong.com" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="https://scholar.google.com/citations?user=-j1j7TkAAAAJ" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                      <a href="https://github.com/ldkong1205/" target="_blank" aria-label="GitHub"> <i class="fab fa-github"></i> </a>
                      <a href="https://www.linkedin.com/in/ldkong/" target="_blank" aria-label="LinkedIn"> <i class="fab fa-linkedin"></i> </a>
                      <a href="https://twitter.com/ldkong1205" target="_blank" aria-label="X (Twitter)"> <i class="fab fa-twitter"> </i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/icons/worldbench-circle.png"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Guoyang Zhao</div>
                      <div class="contributor-note">Contributor, VLA</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/zeying_gong.png"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Zeying Gong</div>
                      <div class="contributor-note">Contributor, VLA</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/jun_cen.jpg"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Jun Cen</div>
                      <div class="contributor-note">Contributor, VLA</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/zhiyu_huang.png"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Zhiyu Huang</div>
                      <div class="contributor-note">Contributor, VLA</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/xiaoshuai_hao.jpg"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Xiaoshuai Hao</div>
                      <div class="contributor-note">Contributor, VLA</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/linfeng_li.jpeg"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Linfeng Li</div>
                      <div class="contributor-note">Contributor, End-to-End Models</div>
                    </div>
                    <div class="contributor-links">
                      <a href="#" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="#" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                      <a href="#" aria-label="GitHub"> <i class="fab fa-github"></i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/icons/worldbench-circle.png"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Hang Song</div>
                      <div class="contributor-note">Contributor, End-to-End Models</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/icons/worldbench-circle.png"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Xiangtai Li</div>
                      <div class="contributor-note">Contributor, End-to-End Models</div>
                    </div>
                    <div class="contributor-links">
                      <a href="https://lxtgh.github.io/" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="https://scholar.google.com/citations?user=FL3ReD0AAAAJ&hl=en" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                      <a href="https://github.com/lxtGH" target="_blank" aria-label="GitHub"> <i class="fab fa-github"></i> </a>
                      <a href="https://twitter.com/xtl994" target="_blank" aria-label="X (Twitter)"> <i class="fab fa-twitter"> </i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/icons/worldbench-circle.png"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Jun Ma</div>
                      <div class="contributor-note">Advisor</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/shaojie_shen.jpeg"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Shaojie Shen</div>
                      <div class="contributor-note">Advisor</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/jianke_zhu.png"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Jianke Zhu</div>
                      <div class="contributor-note">Advisor</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/dacheng_tao.jpeg"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Dacheng Tao</div>
                      <div class="contributor-note">Advisor</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/ziwei_liu.png"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Ziwei Liu</div>
                      <div class="contributor-note">Advisor</div>
                    </div>
                    <div class="contributor-links">
                      <a href="https://liuziwei7.github.io/" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="https://scholar.google.com/citations?user=lc45xlcAAAAJ" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                      <a href="https://github.com/liuziwei7" target="_blank" aria-label="GitHub"> <i class="fab fa-github"></i> </a>
                      <a href="https://sg.linkedin.com/in/ziwei-liu-852b265a" target="_blank" aria-label="LinkedIn"> <i class="fab fa-linkedin"></i> </a>
                      <a href="https://twitter.com/liuziwei7" target="_blank" aria-label="X (Twitter)"> <i class="fab fa-twitter"> </i> </a>
                    </div>
                  </div>

                  <!-- Row -->
                  <div class="contributor-row">
                    <img class="contributor-photo"
                        src="assets_common/photos/junwei_liang.jpeg"
                        loading="lazy" width="56" height="56">

                    <div class="contributor-text">
                      <div class="contributor-name">Junwei Liang</div>
                      <div class="contributor-note">Advisor</div>
                    </div>
                    <div class="contributor-links">
                      <a href="" target="_blank" aria-label="Homepage"><i class="fas fa-globe"></i> </a>
                      <a href="" target="_blank" aria-label="Google Scholar"> <i class="ai ai-google-scholar"></i> </a>
                    </div>
                  </div>

                </div>
                <br>
              </div>
              <br><br>
            </section>




            <section id="related-projects">
              <div class="wb-section">

                <h2>Related Projects</h2>

                <div class="project-grid">

                  <!-- Project -->
                  <article class="wb-card project-card">
                    <div class="project-thumb">
                      <img src="assets_common/teasers/worldlens.png"
                         loading="lazy">
                    </div>
                    <h3 class="project-title">WorldLens: Full-Spectrum Evaluations of Driving World Models</h3>
                    <div class="project-links">
                      <a href="" target="_blank" class="project-link">
                        <i class="ai ai-arxiv"></i>
                      </a>
                      <a href="https://worldbench.github.io/worldlens" target="_blank" class="project-link">
                        <i class="fas fa-globe"></i>
                      </a>
                      <a href="https://github.com/worldbench/WorldLens" target="_blank" class="project-link">
                        <img src="assets_common/icons/github.png" class="icon-img" loading="lazy">
                      </a>
                      <a href="https://huggingface.co/spaces/worldbench/WorldLens" target="_blank" class="project-link">
                        <img src="assets_common/icons/hf.png" class="icon-img" loading="lazy">
                      </a>
                    </div>
                  </article>

                  <!-- Project -->
                  <article class="wb-card project-card">
                    <div class="project-thumb">
                      <img src="assets_common/teasers/worldbench_survey.webp"
                         loading="lazy">
                    </div>
                    <h3 class="project-title">3D and 4D World Modeling: A Survey</h3>
                    <div class="project-links">
                      <a href="https://arxiv.org/abs/2509.07996" target="_blank" class="project-link">
                        <i class="ai ai-arxiv"></i>
                      </a>
                      <a href="https://worldbench.github.io/survey" target="_blank" class="project-link">
                        <i class="fas fa-globe"></i>
                      </a>
                      <a href="https://github.com/worldbench/survey" target="_blank" class="project-link">
                        <img src="assets_common/icons/github.png" class="icon-img" loading="lazy">
                      </a>
                      <a href="https://huggingface.co/spaces/worldbench/WorldLens" target="_blank" class="project-link">
                        <img src="assets_common/icons/hf.png" class="icon-img" loading="lazy">
                      </a>
                    </div>
                  </article>

                  <!-- Project -->
                  <article class="wb-card project-card">
                    <div class="project-thumb">
                      <img src="assets_common/teasers/lidarcrafter.png"
                         loading="lazy">
                    </div>
                    <h3 class="project-title">LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences</h3>
                    <div class="project-links">
                      <a href="https://arxiv.org/abs/2508.03692" target="_blank" class="project-link">
                        <i class="ai ai-arxiv"></i>
                      </a>
                      <a href="https://lidarcrafter.github.io/" target="_blank" class="project-link">
                        <i class="fas fa-globe"></i>
                      </a>
                      <a href="https://github.com/lidarcrafter/toolkit" target="_blank" class="project-link">
                        <img src="assets_common/icons/github.png" class="icon-img" loading="lazy">
                      </a>
                    </div>
                  </article>

                  <!-- Project -->
                  <article class="wb-card project-card">
                    <div class="project-thumb">
                      <img src="assets_common/teasers/3eed.png"
                         loading="lazy">
                    </div>
                    <h3 class="project-title">3EED: Ground Everything Everywhere in 3D</h3>
                    <div class="project-links">
                      <a href="https://arxiv.org/abs/2511.01755" target="_blank" class="project-link">
                        <i class="ai ai-arxiv"></i>
                      </a>
                      <a href="https://project-3eed.github.io/" target="_blank" class="project-link">
                        <i class="fas fa-globe"></i>
                      </a>
                      <a href="https://github.com/worldbench/3EED" target="_blank" class="project-link">
                        <img src="assets_common/icons/github.png" class="icon-img" loading="lazy">
                      </a>
                      <a href="https://huggingface.co/datasets/RRRong/3EED" target="_blank" class="project-link">
                        <img src="assets_common/icons/hf.png" class="icon-img" loading="lazy">
                      </a>
                    </div>
                  </article>

                  <!-- Project -->
                  <article class="wb-card project-card">
                    <div class="project-thumb">
                      <img src="assets_common/teasers/pi3det.png"
                         loading="lazy">
                    </div>
                    <h3 class="project-title">Perspective-Invariant 3D Object Detection</h3>
                    <div class="project-links">
                      <a href="https://arxiv.org/abs/2507.17665" target="_blank" class="project-link">
                        <i class="ai ai-arxiv"></i>
                      </a>
                      <a href="https://pi3det.github.io/" target="_blank" class="project-link">
                        <i class="fas fa-globe"></i>
                      </a>
                      <a href="https://github.com/pi3det/toolkit" target="_blank" class="project-link">
                        <img src="assets_common/icons/github.png" class="icon-img" loading="lazy">
                      </a>
                      <a href="https://huggingface.co/datasets/Pi3DET/data" target="_blank" class="project-link">
                        <img src="assets_common/icons/hf.png" class="icon-img" loading="lazy">
                      </a>
                    </div>
                  </article>

                  <!-- Project -->
                  <article class="wb-card project-card">
                    <div class="project-thumb">
                      <img src="assets_common/teasers/drivebench.png"
                         loading="lazy">
                    </div>
                    <h3 class="project-title">Are VLMs Ready for Autonomous Driving?</h3>
                    <div class="project-links">
                      <a href="https://arxiv.org/abs/2501.04003" target="_blank" class="project-link">
                        <i class="ai ai-arxiv"></i>
                      </a>
                      <a href="https://drive-bench.github.io/" target="_blank" class="project-link">
                        <i class="fas fa-globe"></i>
                      </a>
                      <a href="https://github.com/drive-bench/toolkit" target="_blank" class="project-link">
                        <img src="assets_common/icons/github.png" class="icon-img" loading="lazy">
                      </a>
                      <a href="https://huggingface.co/datasets/drive-bench/arena" target="_blank" class="project-link">
                        <img src="assets_common/icons/hf.png" class="icon-img" loading="lazy">
                      </a>
                    </div>
                  </article>

                  <!-- Project -->
                  <article class="wb-card project-card">
                    <div class="project-thumb">
                      <img src="assets_common/teasers/dynamiccity.webp"
                         loading="lazy">
                    </div>
                    <h3 class="project-title">DynamicCity: Large-Scale 4D Occupancy Generation</h3>
                    <div class="project-links">
                      <a href="https://arxiv.org/abs/2410.18084" target="_blank" class="project-link">
                        <i class="ai ai-arxiv"></i>
                      </a>
                      <a href="https://dynamic-city.github.io/" target="_blank" class="project-link">
                        <i class="fas fa-globe"></i>
                      </a>
                      <a href="https://github.com/3DTopia/DynamicCity" target="_blank" class="project-link">
                        <img src="assets_common/icons/github.png" class="icon-img" loading="lazy">
                      </a>
                    </div>
                  </article>

                  <!-- Project -->
                  <article class="wb-card project-card">
                    <div class="project-thumb">
                      <img src="assets_common/teasers/spiral.png"
                         loading="lazy">
                    </div>
                    <h3 class="project-title">SPIRAL: Semantic-Aware Progressive LiDAR Generation</h3>
                    <div class="project-links">
                      <a href="https://arxiv.org/abs/2505.22643" target="_blank" class="project-link">
                        <i class="ai ai-arxiv"></i>
                      </a>
                      <a href="https://dekai21.github.io/SPIRAL/" target="_blank" class="project-link">
                        <i class="fas fa-globe"></i>
                      </a>
                      <a href="https://github.com/worldbench/SPIRAL" target="_blank" class="project-link">
                        <img src="assets_common/icons/github.png" class="icon-img" loading="lazy">
                      </a>
                    </div>
                  </article>

                  <!-- Project -->
                  <article class="wb-card project-card">
                    <div class="project-thumb">
                      <img src="assets_common/teasers/see4d.png"
                         loading="lazy">
                    </div>
                    <h3 class="project-title">See4D: Pose-Free 4D Generation via Auto-Regressive Inpainting</h3>
                    <div class="project-links">
                      <a href="https://arxiv.org/abs/2510.26796" target="_blank" class="project-link">
                        <i class="ai ai-arxiv"></i>
                      </a>
                      <a href="https://see-4d.github.io/" target="_blank" class="project-link">
                        <i class="fas fa-globe"></i>
                      </a>
                    </div>
                  </article>


                </div>

              </div><br><br>
            </section>


            <footer class="wb-footer">
              <br>
              <div class="wb-footer-content">
                <p>
                  This website is licensed under a
                  <a href="https://creativecommons.org/licenses/by-sa/4.0/"
                    target="_blank"
                    rel="noopener noreferrer">
                    Creative Commons Attribution-ShareAlike 4.0 International License
                  </a>
                </p>
                <p>
                  Â© WorldBench 2025-2026. All Rights Reserved
                </p>
              </div>
              <br>
            </footer>


        </d-article>











        
        <script src="contents_bar.js"></script>
        <script>

          const paperStore = {
            va: {
                A: [  // Action-Only Models
                { title:"Learning by Cheating", authors:"", venue:"CoRL", year:2020, paper:"https://arxiv.org/abs/1912.12294", code:"https://github.com/dotchen/LearningByCheating", project:"#"},
                { title:"End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances", authors:"", venue:"CVPR", year:2020, paper:"https://arxiv.org/abs/1911.10868", code:"#", project:"#"},
                { title:"NEAT: Neural Attention Fields for End-to-End Autonomous Driving", authors:"", venue:"ICCV", year:2021, paper:"https://arxiv.org/abs/2109.04456", code:"https://github.com/autonomousvision/neat", project:"#"},
                { title:"End-to-End Urban Driving by Imitating a Reinforcement Learning Coach (Roach)", authors:"", venue:"ICCV", year:2021, paper:"https://arxiv.org/abs/2108.08265", code:"https://github.com/zhejz/carla-roach", project:"https://zhejz.github.io/roach"},
                { title:"Learning to Drive from A World on Rails (WoR)", authors:"", venue:"ICCV", year:2021, paper:"https://arxiv.org/abs/2105.00636", code:"https://github.com/dotchen/WorldOnRails", project:"https://dotchen.github.io/world_on_rails/"},
                { title:"Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline (TCP)", authors:"", venue:"NeurIPS", year:2022, paper:"https://arxiv.org/abs/2206.08129", code:"https://github.com/OpenDriveLab/TCP", project:"#"},
                { title:"Urban Driver: Learning to Drive from Real-world Demonstrations Using Policy Gradients", authors:"", venue:"CoRL", year:2022, paper:"https://arxiv.org/abs/2109.13333", code:"https://github.com/woven-planet/l5kit", project:"https://woven-planet.github.io/l5kit/urban_driver.html/"},
                { title:"Learning from All Vehicles (LAV)", authors:"", venue:"CVPR", year:2022, paper:"https://arxiv.org/abs/2203.11934", code:"https://github.com/dotchen/LAV", project:"https://dotchen.github.io/LAV/"},
                { title:"TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving", authors:"", venue:"TPAMI", year:2023, paper:"https://arxiv.org/abs/2205.15997", code:"https://github.com/autonomousvision/transfuser", project:"#"},
                { title:"GRI: General Reinforced Imitation and its Application to Vision-Based Autonomous Driving", authors:"", venue:"Robotics", year:2023, paper:"https://arxiv.org/abs/2111.08575", code:"#", project:"#"},
                { title:"Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving? (BEVPlanner)", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2312.03031", code:"https://github.com/NVlabs/BEV-Planner", project:"#"},
                { title:"Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.16394", code:"#", project:"#"},
                { title:"GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2503.05689", code:"https://github.com/YvanYin/GoalFlow", project:"https://zebinx.github.io/HomePage-of-GoalFlow/"},
                { title:"RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2502.13144", code:"#", project:"https://hgao-cv.github.io/RAD/"}
                ],
                B: [  // Perception-Action Models
                { title:"ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning", authors:"", venue:"ECCV", year:2022, paper:"https://arxiv.org/abs/2207.07601", code:"https://github.com/OpenDriveLab/ST-P3", project:"#"},
                { title:"Planning-oriented Autonomous Driving (UniAD)", authors:"", venue:"CVPR", year:2023, paper:"https://arxiv.org/abs/2212.10156", code:"https://github.com/OpenDriveLab/UniAD", project:"#"},
                { title:"VAD: Vectorized Scene Representation for Efficient Autonomous Driving", authors:"", venue:"ICCV", year:2023, paper:"https://arxiv.org/abs/2303.12077", code:"https://github.com/hustvl/VAD", project:"#"},
                { title:"Scene as Occupancy (OccNet)", authors:"", venue:"ICCV", year:2023, paper:"https://arxiv.org/abs/2306.02851", code:"https://github.com/OpenDriveLab/OccNet", project:"#"},
                { title:"GenAD: Generative End-to-End Autonomous Driving", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2402.11502", code:"https://github.com/wzzheng/GenAD", project:"#"},
                { title:"PARA-Drive: Parallelized Architecture for Real-time Autonomous Driving", authors:"", venue:"CVPR", year:2024, paper:"https://openaccess.thecvf.com/content/CVPR2024/papers/Weng_PARA-Drive_Parallelized_Architecture_for_Real-time_Autonomous_Driving_CVPR_2024_paper.pdf", code:"#", project:"https://xinshuoweng.github.io/paradrive/"},
                { title:"SparseAD: Sparse Query-Centric Paradigm for Efficient End-to-End Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2404.06892", code:"#", project:"#"},
                { title:"GaussianAD: Gaussian-Centric End-to-End Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.10371", code:"#", project:"#"},
                { title:"DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and Iterative Refinement for Efficient End-to-End Self-Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2409.09777", code:"https://github.com/suhaisheng/DiFSD", project:"#"},
                { title:"DriveTransformer: Unified Transformer for Scalable End-to-End Autonomous Driving", authors:"", venue:"ICLR", year:2025, paper:"https://arxiv.org/abs/2503.07656", code:"https://github.com/Thinklab-SJTU/DriveTransformer", project:"#"},
                { title:"SparseDrive: End-to-End Autonomous Driving via Sparse Scene Representation", authors:"", venue:"ICRA", year:2025, paper:"https://arxiv.org/abs/2405.19620", code:"https://github.com/swc-17/SparseDrive", project:"#"},
                { title:"DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2411.15139", code:"https://github.com/hustvl/DiffusionDrive", project:"#"}
                ],
                C: [  // Image-Based World Models
                { title:"DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2309.09777", code:"https://github.com/JeffWang987/DriveDreamer", project:"https://drivedreamer.github.io/"},
                { title:"GenAD: Generalized Predictive Model for Autonomous Driving", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2403.09630", code:"https://github.com/OpenDriveLab/DriveAGI", project:"#"},
                { title:"Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving (Drive-WM)", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2311.17918", code:"https://github.com/BraveGroup/Drive-WM", project:"https://drive-wm.github.io/"},
                { title:"DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.19505", code:"https://github.com/YvanYin/DrivingWorld", project:"https://huxiaotaostasy.github.io/DrivingWorld/index.html"},
                { title:"Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies", authors:"", venue:"IROS", year:2025, paper:"https://arxiv.org/abs/2411.10171", code:"#", project:"https://imagine-2-drive.github.io/"},
                { title:"DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.18607", code:"#", project:"https://rogerchern.github.io/DrivingGPT/"},
                { title:"Epona: Autoregressive Diffusion World Model for Autonomous Driving", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2506.24113", code:"https://github.com/Kevin-thu/Epona", project:"https://kevin-thu.github.io/Epona/"}
                ],
                D: [  // Occupancy World Models
                { title:"OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2311.16038", code:"https://github.com/wzzheng/OccWorld", project:"https://wzzheng.net/OccWorld/"},
                { title:"Neural Volumetric World Models for Autonomous Driving (NeMo)", authors:"", venue:"ECCV", year:2024, paper:"https://eccv.ecva.net/virtual/2024/poster/250", code:"#", project:"#"},
                { title:"OCCVAR: Scalable 4D Occupancy Prediction via Next-Scale Prediction", authors:"", venue:"OpenReview", year:2024, paper:"https://openreview.net/forum?id=X2HnTFsFm8", code:"#", project:"#"},
                { title:"RenderWorld: World Model with Self-Supervised 3D Label", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2409.11356", code:"#", project:"#"},
                { title:"An Efficient Occupancy World Model via Decoupled Dynamic Flow and Image-assisted Training (DFIT-OccWorld)", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.13772", code:"#", project:"#"},
                { title:"Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving (Drive-OccWorld)", authors:"", venue:"AAAI", year:2025, paper:"https://arxiv.org/abs/2408.14197", code:"https://github.com/yuyang-cloud/Drive-OccWorld", project:"https://drive-occworld.github.io/"},
                { title:"Temporal Triplane Transformers as Occupancy World Models (TÂ³Former)", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.07338", code:"#", project:"#"}
                ],
                E: [  // Latent World Models
                { title:"Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2409.16663", code:"#", project:"#"},
                { title:"World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2507.00603", code:"#", project:"#"},
                { title:"End-to-End Driving with Online Trajectory Evaluation via BEV World Model (WoTE)", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2504.01941", code:"https://github.com/liyingyanUCAS/WoTE", project:"#"},
                { title:"Enhancing End-to-End Autonomous Driving with Latent World Model (LAW)", authors:"", venue:"ICLR", year:2025, paper:"https://arxiv.org/abs/2406.08481", code:"https://github.com/BraveGroup/LAW", project:"#"},
                { title:"Navigation-Guided Sparse Scene Representation for End-to-End Autonomous Driving (SSR)", authors:"", venue:"ICLR", year:2025, paper:"https://arxiv.org/abs/2409.18341", code:"https://github.com/PeidongLi/SSR", project:"#"},
                { title:"Echo Planning for Autonomous Driving: From Current Observations to Future Trajectories and Back", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.18945", code:"#", project:"#"},
                { title:"Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution (SeerDrive)", authors:"", venue:"NeurIPS", year:2025, paper:"https://arxiv.org/abs/2510.11092", code:"https://github.com/LogosRoboticsGroup/SeerDrive", project:"#"}
                ]
            },
            
            // Vision-Language-Action Models
            vla: {
                A: [  // Textual Action Generator
                { title:"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", authors:"", venue:"arXiv", year:2023, paper:"https://arxiv.org/abs/2312.09245", code:"https://github.com/OpenGVLab/DriveMLM", project:"#"},
                { title:"Drive Like a Human: Rethinking Autonomous Driving with Large Language Models", authors:"", venue:"arXiv", year:2023, paper:"https://arxiv.org/abs/2307.07162", code:"https://github.com/PJLab-ADG/DriveLikeAHuman", project:"https://vilonge.github.io/OccLLaMA_Page/"},
                { title:"GPT-Driver: Learning to Drive with GPT", authors:"", venue:"NeurIPSW", year:2023, paper:"https://arxiv.org/abs/2310.01415", code:"https://github.com/PointsCoder/GPT-Driver", project:"https://pointscoder.github.io/projects/gpt_driver/index.html"},
                { title:"RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model", authors:"", venue:"RSS", year:2024, paper:"https://arxiv.org/abs/2402.10828", code:"https://github.com/YuanJianhao508/RAG-Driver", project:"https://yuanjianhao508.github.io/RAG-Driver/"},
                { title:"Making Large Language Models Better Planners with Reasoning-Decision Alignment (RDA-Driver)", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2408.13890", code:"#", project:"#"},
                { title:"DriveLM: Driving with Graph Visual Question Answering", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2312.14150", code:"https://github.com/OpenDriveLab/DriveLM", project:"https://opendrivelab.com/DriveLM/"},
                { title:"DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model", authors:"", venue:"RA-L", year:2024, paper:"https://arxiv.org/abs/2310.01412", code:"#", project:"https://tonyxuqaq.github.io/projects/DriveGPT4/"},
                { title:"DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experience", authors:"", venue:"IROS", year:2024, paper:"https://arxiv.org/abs/2406.03008", code:"https://github.com/sled-group/driVLMe", project:"https://sled-group.github.io/driVLMe/"},
                { title:"Driving Everywhere with Large Language Model Policy Adaptation (LLaDA)", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2402.05932", code:"https://github.com/Boyiliee/LLaDA-AV", project:"https://boyiliee.github.io/llada/"},
                { title:"VLAAD: Vision and Language Assistant for Autonomous Driving", authors:"", venue:"WACVW", year:2024, paper:"https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/papers/Park_VLAAD_Vision_and_Language_Assistant_for_Autonomous_Driving_WACVW_2024_paper.pdf", code:"https://github.com/sungyeonparkk/vision-assistant-for-driving", project:"#"},
                { title:"OccLLaMA: A Unified Occupancy-Language-Action World Model for Understanding and Generation Tasks in Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2409.03272", code:"#", project:"https://vilonge.github.io/OccLLaMA_Page/"},
                { title:"Doe-1: Closed-Loop Autonomous Driving with Large World Model", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.09627", code:"https://github.com/wzzheng/Doe", project:"https://wzzheng.net/Doe/"},
                { title:"SafeAuto: Knowledge-Enhanced Safe Autonomous Driving with Multimodal Foundation Models", authors:"", venue:"ICML", year:2025, paper:"https://arxiv.org/abs/2503.00211", code:"https://github.com/AI-secure/SafeAuto", project:"#"},
                { title:"OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.15208", code:"https://github.com/taco-group/OpenEMMA", project:"#"},
                { title:"ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving", authors:"", venue:"CoRL", year:2025, paper:"https://arxiv.org/abs/2505.20024", code:"https://github.com/Liuxueyi/ReasonPlan", project:"#"},
                { title:"FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", authors:"", venue:"NeurIPS", year:2025, paper:"https://arxiv.org/abs/2505.17685", code:"https://github.com/MIV-XJTU/FSDrive", project:"https://miv-xjtu.github.io/FSDrive.github.io/"},
                { title:"World knowledge-enhanced Reasoning Using Instruction-guided Interactor in Autonomous Driving (WKER)", authors:"", venue:"AAAI", year:2025, paper:"https://arxiv.org/abs/2412.06324", code:"#", project:"#"},
                { title:"OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2405.01533", code:"https://github.com/NVlabs/OmniDrive", project:"#"},
                { title:"EMMA: End-to-End Multimodal Model for Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2410.23262", code:"#", project:"https://waymo.com/blog/2024/10/introducing-emma/"},
                { title:"AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.07608", code:"https://github.com/hustvl/AlphaDrive", project:"#"},
                { title:"Occ-LLM: Enhancing Autonomous Driving with Occupancy-Based Large Language Models", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2502.06419", code:"#", project:"#"},
                { title:"DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2507.20879", code:"#", project:"#"},
                { title:"Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2502.14917", code:"#", project:"#"},
                { title:"Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.18234", code:"#", project:"#"},
                { title:"FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2507.23318", code:"#", project:"#"},
                { title:"WiseAD: Knowledge Augmented End-to-End Autonomous Driving with Vision-Language Model", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.09951", code:"https://github.com/wyddmw/WiseAD", project:"https://wyddmw.github.io/WiseAD_demo/"},
                { title:"Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.23757", code:"https://github.com/ahydchh/Impromptu-VLA", project:"https://impromptu-vla.c7w.tech/"},
                { title:"LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.00284", code:"https://github.com/michigan-traffic-lab/LightEMMA", project:"#"},
                { title:"AutoDrive-RÂ²: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2509.01944", code:"#", project:"#"},
                { title:"OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2509.00789", code:"#", project:"#"}
                ],
                B: [  // Numerical Action Generator
                { title:"LMDrive: Closed-Loop End-to-End Driving with Large Language Models", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2312.07488", code:"https://github.com/opendilab/LMDrive", project:"https://hao-shao.com/projects/lmdrive.html"},
                { title:"LINGO-2: Driving with Natural Language", authors:"", venue:"", year:2024, paper:"#", code:"#", project:"https://wayve.ai/thinking/lingo-2-driving-with-language/"},
                { title:"CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving (CoVLA-Agent)", authors:"", venue:"WACV", year:2025, paper:"https://arxiv.org/abs/2408.10845", code:"#", project:"https://turingmotors.github.io/covla-ad/"},
                { title:"ORION: A Holistic End-to-End Autonomous Driving Framework by Vision-Language Instructed Action Generation", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2503.19755", code:"https://github.com/xiaomi-mlab/Orion", project:"https://xiaomi-mlab.github.io/Orion/"},
                { title:"AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", authors:"", venue:"NeurIPS", year:2025, paper:"https://arxiv.org/abs/2506.13757", code:"https://github.com/ucla-mobility/AutoVLA", project:"https://autovla.github.io/"},
                { title:"SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2503.09594", code:"https://github.com/RenzKa/simlingo", project:"https://www.katrinrenz.de/simlingo/"},
                { title:"DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving", authors:"", venue:"CVPR", year:2025, paper:"https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous_CVPR_2025_paper.pdf", code:"#", project:"#"},
                { title:"S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Model with Spatio-Temporal Visual Representation", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2505.24139", code:"#", project:"https://s4-driver.github.io/"},
                { title:"VaViM and VaVAM: Autonomous Driving through Video Generative Modeling", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2502.15672", code:"https://github.com/valeoai/VideoActionModel", project:"https://valeoai.github.io/vavim-vavam/"},
                { title:"BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.03074", code:"#", project:"#"},
                { title:"DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.16278", code:"https://github.com/Thinklab-SJTU/DriveMoE", project:"https://thinklab-sjtu.github.io/DriveMoE/"},
                { title:"DSDrive: Distilling Large Language Model for Lightweight End-to-End Autonomous Driving with Unified Reasoning and Planning", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.05360", code:"#", project:"#"},
                { title:"OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.23463", code:"https://github.com/DriveVLA/OpenDriveVLA", project:"https://drivevla.github.io/"},
                { title:"A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving (PLA)", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2507.23540", code:"#", project:"#"},
                { title:"OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2509.05578", code:"#", project:"#"}
                ],
                C: [  // Explicit Action Guidance
                { title:"DriveVLM: The convergence of autonomous driving and large vision-language models", authors:"", venue:"CoRL", year:2024, paper:"https://arxiv.org/abs/2402.12289", code:"#", project:"https://tsinghua-mars-lab.github.io/DriveVLM/"},
                { title:"DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving", authors:"", venue:"IROS", year:2025, paper:"https://arxiv.org/abs/2409.18053", code:"https://github.com/TUM-AVS/DualAD", project:"https://dualad.github.io/"},
                { title:"FASIONAD: FAst and Slow FusION Thinking Systems for Human-Like Autonomous Driving with Adaptive Feedback", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2411.18013", code:"#", project:"#"},
                { title:"Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2410.22313", code:"https://github.com/hustvl/Senna", project:"#"},
                { title:"Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving (LeapAD)", authors:"", venue:"NeurIPS", year:2024, paper:"https://arxiv.org/abs/2405.15324", code:"https://github.com/PJLab-ADG/LeapAD", project:"https://pjlab-adg.github.io/LeapAD/"},
                { title:"DME-Driver: Integrating human decision logic and 3D scene perception in autonomous driving", authors:"", venue:"AAAI", year:2025, paper:"https://arxiv.org/abs/2401.03641", code:"#", project:"#"},
                { title:"SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2505.16805", code:"#", project:"#"},
                { title:"FASIONAD++: Integrating High-Level Instruction and Information Bottleneck in FAt-Slow fusION Systems for Enhanced Safety in Autonomous Driving with Adaptive Feedback", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.08162", code:"#", project:"#"},
                { title:"LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and Dual-Process Thinking", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2501.08168", code:"#", project:"#"},
                { title:"DiffVLA: Vision-language guided diffusion planning for autonomous driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.19381", code:"#", project:"#"},
                { title:"ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2507.12499", code:"#", project:"https://4dvlab.github.io/project_page/realad"}
                ],
                D: [  // Implicit Representations Transfer
                { title:"VLP: Vision Language Planning for Autonomous Driving", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2401.05577", code:"#", project:"#"},
                { title:"VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision", authors:"", venue:"CoRL", year:2025, paper:"https://arxiv.org/abs/2412.14446", code:"#", project:"#"},
                { title:"Distilling Multi-modal Large Language Models for Autonomous Driving (DiMA)", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2501.09757", code:"#", project:"#"},
                { title:"ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.15158", code:"#", project:"#"},
                { title:"VERDI: VLM-Embedded Reasoning for Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.15925", code:"#", project:"#"},
                { title:"VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2502.18042", code:"#", project:"#"},
                { title:"ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.08052", code:"https://github.com/xiaomi-research/ReCogDrive", project:"https://xiaomi-research.github.io/recogdrive/"},
                { title:"InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.13047", code:"https://github.com/ruiqi-song/InsightDrive", project:"#"},
                { title:"NetRoller: Interfacing General and Specialized Models for End-to-End Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.14589", code:"https://github.com/Rex-sys-hk/NetRoller", project:"#"},
                { title:"ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.07725", code:"https://github.com/OpenDriveLab/ETA", project:"#"},
                { title:"ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2508.12603", code:"#", project:"#"}
                ]
            },
            dataset: {
                A: [  // Vision-Action Datasets
                { title:"BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning", authors:"", venue:"CVPR", year:2020, paper:"https://arxiv.org/abs/1805.04687", code:"https://github.com/bdd100k/bdd100k", project:"https://bair.berkeley.edu/blog/2018/05/30/bdd/"},
                { title:"nuScenes: A Multimodal Dataset for Autonomous Driving", authors:"", venue:"CVPR", year:2020, paper:"https://arxiv.org/abs/1903.11027", code:"#", project:"https://www.nuscenes.org/"},
                { title:"Scalability in Perception for Autonomous Driving: Waymo Open Dataset", authors:"", venue:"CVPR", year:2020, paper:"https://arxiv.org/abs/1912.04838", code:"https://github.com/waymo-research/waymo-open-dataset", project:"https://waymo.com/open/"},
                { title:"nuPlan: A Closed-Loop ML-Based Planning Benchmark for Autonomous Vehicles", authors:"", venue:"arXiv", year:2021, paper:"https://arxiv.org/abs/2106.11810", code:"https://github.com/motional/nuplan-devkit", project:"https://www.nuplan.org/"},
                { title:"Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting", authors:"", venue:"NeurIPS", year:2021, paper:"https://arxiv.org/abs/2301.00493", code:"https://github.com/argoverse/av2-api", project:"https://www.argoverse.org/av2.html"},
                { title:"Bench2Drive: Towards multi-ability benchmarking of closed-loop end-to-end autonomous driving", authors:"", venue:"NeurIPS", year:2024, paper:"https://arxiv.org/abs/2406.03877", code:"https://github.com/Thinklab-SJTU/Bench2Drive", project:"#"},
                { title:"Benchmarking and Improving Bird's Eye View Perception Robustness in Autonomous Driving", authors:"", venue:"TPAMI", year:2025, paper:"https://arxiv.org/abs/2405.17426", code:"https://github.com/Daniel-xsy/RoboBEV", project:"#"}
                ],
                B: [  // Vision-Language-Action Datasets
                { title:"Textual explanations for self-driving vehicles (BDD-X)", authors:"", venue:"ECCV", year:2018, paper:"https://arxiv.org/abs/1807.11546", code:"https://github.com/JinkyuKimUCB/BDD-X-dataset", project:"#"},
                { title:"Talk2Car: Predicting physical trajectories for natural language commands", authors:"", venue:"IEEE Access", year:2022, paper:"https://ieeexplore.ieee.org/document/9961196", code:"https://github.com/ThierryDeruyttere/Talk2Car-Trajectory", project:"#"},
                { title:"DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents (SDN)", authors:"", venue:"EMNLP", year:2022, paper:"https://arxiv.org/abs/2210.12511", code:"https://github.com/sled-group/DOROTHIE", project:"#"},
                { title:"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", authors:"", venue:"arXiv", year:2023, paper:"https://arxiv.org/abs/2312.09245", code:"https://github.com/OpenGVLab/DriveMLM", project:"#"},
                { title:"LMDrive: Closed-Loop End-to-End Driving with Large Language Models", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2312.07488", code:"https://github.com/opendilab/LMDrive", project:"https://hao-shao.com/projects/lmdrive.html"},
                { title:"DriveLM: Driving with graph visual question answering (DriveLM-nuScenes & DriveLM-CARLA)", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2312.14150", code:"https://github.com/OpenDriveLab/DriveLM", project:"https://opendrivelab.com/DriveLM/"},
                // { title:"DriveLM: Driving with graph visual question answering (DriveLM-CARLA)", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2312.14150", code:"https://github.com/OpenDriveLab/DriveLM", project:"https://opendrivelab.com/DriveLM/"},
                { title:"DME-Driver: Integrating human decision logic and 3D scene perception in autonomous driving (HBD)", authors:"", venue:"AAAI", year:2025, paper:"https://arxiv.org/abs/2401.03641", code:"#", project:"#"},
                { title:"VLAAD: Vision and Language Assistant for Autonomous Driving", authors:"", venue:"WACVW", year:2024, paper:"https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/papers/Park_VLAAD_Vision_and_Language_Assistant_for_Autonomous_Driving_WACVW_2024_paper.pdf", code:"https://github.com/sungyeonparkk/vision-assistant-for-driving", project:"#"},
                { title:"DriveVLM: The convergence of autonomous driving and large vision-language models (SUP-AD)", authors:"", venue:"CoRL", year:2024, paper:"https://arxiv.org/abs/2402.12289", code:"#", project:"https://tsinghua-mars-lab.github.io/DriveVLM/"},
                { title:"Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models (NuInstruct)", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2401.00988", code:"https://github.com/xmed-lab/NuInstruct", project:"#"},
                { title:"WOMD-Reasoning: A large-scale dataset for interaction reasoning in driving", authors:"", venue:"ICML", year:2025, paper:"https://arxiv.org/abs/2407.04281", code:"https://github.com/yhli123/WOMD-Reasoning", project:"https://waymo.com/open/download"},
                { title:"DriveCoT: Integrating chain-of-thought reasoning with end-to-end driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2403.16996", code:"#", project:"https://drivecot.github.io/"},
                { title:"Reason2Drive: Towards interpretable and chain-based reasoning for autonomous driving", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2312.03661", code:"https://github.com/fudan-zvg/Reason2Drive", project:"#"},
                { title:"Are VLMs ready for autonomous driving? an empirical study from the reliability, data, and metric perspectives (DriveBench)", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2501.04003", code:"https://github.com/drive-bench/toolkit", project:"https://drive-bench.github.io/"},
                { title:"AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning (MetaAD)", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.07608", code:"https://github.com/hustvl/AlphaDrive", project:"https://drive-bench.github.io/"},
                { title:"OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2405.01533", code:"https://github.com/NVlabs/OmniDrive", project:"#"},
                { title:"Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving (NuInteract)", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.08725", code:"#", project:"#"},
                { title:"DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.05667", code:"#", project:"#"},
                { title:"Impromptu VLA: Open weights and open data for driving vision-language-action models", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.23757", code:"https://github.com/ahydchh/Impromptu-VLA", project:"https://impromptu-vla.c7w.tech/"},
                { title:"CoVLA: Comprehensive vision-language-action dataset for autonomous driving", authors:"", venue:"WACV", year:2025, paper:"https://arxiv.org/abs/2408.10845", code:"#", project:"https://turingmotors.github.io/covla-ad/"},
                { title:"OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving (OmniReason-nuScenes & OmniReason-B2D)", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2509.00789", code:"#", project:"#"},
                // { title:"OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving (OmniReason-B2D)", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2509.00789", code:"#", project:"#"}
                ]
            },            
          };
    
          (function initAllPapersSections(){
            document.querySelectorAll('.papers-section').forEach(section => initPapersSection(section));
          })();
    
          function initPapersSection(root){
            const sectionKey = root.getAttribute('data-section-key');
            const tabs = Array.from(root.querySelectorAll('.tab-btn'));
            const panels = {
                A: root.querySelector(`[id$="panel-a"]`),
                B: root.querySelector(`[id$="panel-b"]`),
                C: root.querySelector(`[id$="panel-c"]`),
                D: root.querySelector(`[id$="panel-d"]`),
                E: root.querySelector(`[id$="panel-e"]`)
            };
            const lists = {
                A: panels.A?.querySelector('.list'),
                B: panels.B?.querySelector('.list'),
                C: panels.C?.querySelector('.list'),
                D: panels.D?.querySelector('.list'),
                E: panels.E?.querySelector('.list')
            };
            const countEl = root.querySelector('.count');
            const searchInput = root.querySelector('.paper-search');

            let activeKey = 'A';
            let keyword = '';

            function makeLink(label, href){
                const a = document.createElement('a');
                a.className = 'link';
                a.textContent = label;
                a.href = href || '#';
                a.target = '_blank';
                a.rel = 'noopener noreferrer';
                return a;
            }
            
            function makeCard(item, badgeIdx=1){
                const wrap = document.createElement('article');
                wrap.className = 'card';

                const dot = document.createElement('span');
                dot.className = 'badge' + (badgeIdx===2?' badge-2': (badgeIdx===3?' badge-3':(badgeIdx===4?' badge-4':(badgeIdx===5?' badge-5':''))));
                wrap.appendChild(dot);

                const box = document.createElement('div');
                const h3 = document.createElement('h3');
                h3.textContent = item.title || 'Untitled';
                const meta = document.createElement('div');
                meta.className = 'meta';
                meta.textContent = [
                item.authors || '',
                item.venue ? `${item.venue}${item.year?` Â· ${item.year}`:''}` : (item.year||'')
                ].filter(Boolean).join(' | ');

                const links = document.createElement('div');
                links.className = 'links';
                links.appendChild(makeLink('Paper', item.paper));
                links.appendChild(makeLink('Code', item.code));
                links.appendChild(makeLink('Project', item.project));

                box.appendChild(h3);
                box.appendChild(meta);
                box.appendChild(links);
                wrap.appendChild(box);
                return wrap;
            }

            const normalize = s => (s||'').toLowerCase();
            function match(item, kw){
                if(!kw) return true;
                const bag = [item.title, item.authors, item.venue, item.year]
                .map(x=>String(x??'')).join(' ').toLowerCase();
                return bag.includes(kw);
            }

            function renderList(subKey){
                const INITIAL = Number(root.dataset.initial || 3);
                const data = (paperStore[sectionKey] && paperStore[sectionKey][subKey]) || [];
                const items = data.filter(it=>match(it, keyword));

                const panelEl = panels[subKey];
                const listEl  = lists[subKey];
                
                if(!panelEl || !listEl) return;

                let ctrlEl = panelEl.querySelector('.showmore-wrap');
                if(!ctrlEl){
                ctrlEl = document.createElement('div');
                ctrlEl.className = 'showmore-wrap';
                panelEl.appendChild(ctrlEl);
                }

                if(panelEl.dataset.expanded === undefined){
                panelEl.dataset.expanded = "false";
                }
                const expanded = panelEl.dataset.expanded === "true";

                const shown = expanded ? items : items.slice(0, INITIAL);

                listEl.innerHTML = '';
                if(items.length===0){
                const empty = document.createElement('div');
                empty.className = 'empty';
                empty.textContent = 'No data or no match.';
                listEl.appendChild(empty);
                ctrlEl.innerHTML = '';
                countEl.textContent = `0 items`;
                return;
                }
                
                shown.forEach(it => listEl.appendChild(makeCard(
                it, subKey==='A'?1: subKey==='B'?2: subKey==='C'?3: subKey==='D'?4:5
                )));

                countEl.textContent = `${shown.length}/${items.length} items`;

                if(items.length > INITIAL){
                ctrlEl.innerHTML = `
                <button class="showmore-btn" type="button">
                    ${expanded ? "Collapse" : `Expand (${items.length - INITIAL})`}
                </button>`;
                const btn = ctrlEl.querySelector('button');
                btn.onclick = () => {
                    panelEl.dataset.expanded = expanded ? "false" : "true";
                    renderList(subKey);
                };
                }else{
                ctrlEl.innerHTML = '';
                }
            }

            function switchTo(subKey){
                activeKey = subKey;
                if(panels[activeKey]) {
                panels[activeKey].dataset.expanded = "false";
                }
                tabs.forEach(btn=>{
                const isSelected = btn.id.endsWith(subKey.toLowerCase());
                btn.setAttribute('aria-selected', String(isSelected));
                });
                Object.entries(panels).forEach(([k,el])=>{
                if(el) {
                    el.setAttribute('aria-hidden', String(k!==subKey));
                }
                });
                renderList(subKey);
            }

            // events
            tabs.forEach(btn=>{
                btn.addEventListener('click', ()=>{
                const subKey = btn.id.split('-').pop().toUpperCase();
                switchTo(subKey);
                });
            });
            
            root.addEventListener('keydown', (e)=>{
                if(e.key==='ArrowRight' || e.key==='ArrowLeft'){
                e.preventDefault();
                const order = ['A','B','C','D','E'].filter(k => panels[k]);
                const idx = order.indexOf(activeKey);
                const len = order.length;
                const next = e.key==='ArrowRight' ? (idx+1)%len : (idx-1+len)%len;
                switchTo(order[next]);
                root.querySelector(`[id$="tab-${order[next].toLowerCase()}"]`)?.focus();
                }
            });
            
            searchInput.addEventListener('input', ()=>{
                keyword = normalize(searchInput.value);
                renderList(activeKey);
            });
            
            searchInput.addEventListener('search', ()=>{
                keyword = normalize(searchInput.value);
                renderList(activeKey);
            });

            // init
            switchTo('A');
            }
        </script>

        <script>
            document.querySelectorAll('.videogen').forEach((root) => {
            const heroVideo  = root.querySelector('[data-hero]');
            const heroSource = heroVideo.querySelector('source');
            const heroLabel  = root.querySelector('[data-hero-label]');
            const thumbs     = Array.from(root.querySelectorAll('.thumb'));
        
            function setActive(btn) {
                thumbs.forEach(b => {
                const on = b === btn;
                b.classList.toggle('is-active', on);
                b.setAttribute('aria-selected', on ? 'true' : 'false');
                });
            }
        
            function switchTo(btn) {
                const src = btn.getAttribute('data-src');
                if (!src) return;
                const label = btn.getAttribute('data-label') || 'Video';
                heroSource.src = src;
                heroVideo.load();
                heroVideo.play().catch(()=>{});
                if (heroLabel) heroLabel.textContent = label;
                setActive(btn);
            }
        
            thumbs.forEach(btn => btn.addEventListener('click', () => switchTo(btn)));
        
            root.addEventListener('keydown', (e) => {
                if (e.key !== 'ArrowLeft' && e.key !== 'ArrowRight') return;
                const idx = thumbs.findIndex(b => b.classList.contains('is-active'));
                if (idx === -1) return;
                const next = e.key === 'ArrowRight'
                ? (idx + 1) % thumbs.length
                : (idx - 1 + thumbs.length) % thumbs.length;
                switchTo(thumbs[next]);
                thumbs[next].scrollIntoView({ behavior: 'smooth', inline: 'center', block: 'nearest' });
            });
        
            const initial = root.querySelector('.thumb.is-active');
            if (initial) switchTo(initial);
            });
        </script>
  
        <script>
          (function(){
            function buildSrc(client, base, method, file, query){
              const slash = base.endsWith('/') ? '' : '/';
              return `${client}?playbackPath=${base}${slash}${method}/${file}${query || ''}`;
            }
          
            document.querySelectorAll('.data-switcher').forEach((root)=>{
              const client = root.dataset.client;
              const base   = root.dataset.base;
              const query  = root.dataset.query || '';
              const iframes = Array.from(root.querySelectorAll('iframe[data-file]'));
              const btns    = Array.from(root.querySelectorAll('.method-btn'));
          
              if(!client || !base || iframes.length===0 || btns.length===0) return;
          
              let inited = false;
          
              function setActive(btn){
                btns.forEach(b=>{
                  const on = b===btn;
                  b.classList.toggle('is-active', on);
                  b.setAttribute('aria-selected', on ? 'true' : 'false');
                });
              }
          
              function switchMethod(btn){
                const method = btn.getAttribute('data-method');
                if(!method) return;
                iframes.forEach(ifr=>{
                  const file = ifr.getAttribute('data-file');
                  if(!file) return;
                  const next = buildSrc(client, base, method, file, query);
                  if(ifr.src !== next){
                    ifr.src = next;
                  }
                });
                setActive(btn);
              }
          
              function initOnce(){
                if(inited) return;
                inited = true;
                const first = root.querySelector('.method-btn.is-active') || btns[0];
                if(first) switchMethod(first);
              }
          
              btns.forEach(btn=>btn.addEventListener('click', ()=>switchMethod(btn)));
          
              root.addEventListener('toggle', ()=>{
                if(root.open) initOnce();
              });
              if(root.open) initOnce();
          
              root.tabIndex = 0;
              root.addEventListener('keydown', (e)=>{
                if(e.key!=='ArrowLeft' && e.key!=='ArrowRight') return;
                const idx = btns.findIndex(b=>b.classList.contains('is-active'));
                if(idx < 0) return;
                const next = e.key==='ArrowRight' ? (idx+1)%btns.length : (idx-1+btns.length)%btns.length;
                switchMethod(btns[next]);
                btns[next].scrollIntoView({ behavior:'smooth', inline:'center', block:'nearest' });
              });
            });
          })();
          </script>

    </body>

</html>