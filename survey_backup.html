<!doctype html>
<html lang="en">
    <head>
        <title>WorldBench</title>
        <link rel="icon" type="image/x-icon" href="/static/img/ButtleFly.ico">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://worldbench.github.io/" />
        <meta property="og:image" content="https://worldbench.github.io/static/img/ButtleFly_3.png" />
        <meta property="og:title" content="DynamicVerse: Physically-Aware Multimodal Modeling for Dynamic 4D Worlds" />
        <meta property="og:description" content="is a physical-scale, multi-modal 4D modeling framework for real-world video, which contains a novel automated data curation pipeline and corresponding large-scale 4D dataset." />

        <!-- Twitter -->
        <meta name="twitter:url" content="https://worldbench.github.io/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="https://worldbench.github.io/static/img/ButtleFly_3.png" />
        <meta name="twitter:title" content="DynamicVerse: Physically-Aware Multimodal Modeling for Dynamic 4D Worlds." />
        <meta name="twitter:description" content="We introduce DynamicVerse, a physical-scale, multi-modal 4D modeling framework for real-world video, which contains a novel automated data curation pipeline and corresponding large-scale 4D dataset." />

        <script src="./static/js/distill_template.v2.js"></script>
        <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>


        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>

        <!-- Scoped styles for the Papers section -->
    </head>
    <body>
        <div class="header-wrapper">

            <div class="header-container", id="header-container">
                <div class="header-content">
                <h1>WorldBench:<br>Benchmarking 3D and 4D World Models in the Real World</h1>
                <div class="button-container">
                    <!-- replace arxiv -->
                    <a href="https://worldbench.github.io/" class="button paper-link" target="_blank">
                        <span class="icon is-small">
                            <i class="ai ai-arxiv "  style="height: 1.5em;"></i>
                        </span>
                        ArXiv
                    </a>
                    <!-- replace pdf -->
                    <a href="https://worldbench.github.io/" class="button paper-link" target="_blank">
                        <span class="icon is-small"  style="height: 1.5em;">
                            <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>PDF</span>
                    </a>
                    <!-- replace image -->
                    <a href="https://github.com/kairunwen/DynamicVerse" class="button" target="_blank">
                        <span class="icon is-small"  style="height: 1.5em;">
                            <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                    </a>

                    <a href="https://huggingface.co/datasets/kairunwen/DynamicVerse" class="button" target="_blank">
                        <span class="icon is-small">
                            <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1.5em;">
                        </span>
                        <span>Dataset</span>
                    </a>
                </div>
                </div>
                <div class="header-image">
                    <img src="images/teaser_eye.png" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>

        <d-article>
            <d-contents>
                <nav>
                    <h4>Contents</h4>
                    <div><a href="#definition">Definition</a></div>
                    <div><a href="#taxonomy">Taxonomy</a></div>
                    <div><a href="#datasets">Datasets</a></div>
                    <div><a href="#evaluatons">Evaluations</a></div>
                    <div><a href="#applications">Applications</a></div>
                    <!-- Added: Papers section -->
                    <div><a href="#papers">Papers</a></div>
                </nav>
            </d-contents>


            <!-- ---------- Introduction (new) ---------- -->
              <section id="introduction">
                <style>

                  /* Scoped styles (won't leak much due to wb- prefix) */
                  .wb-section {
                    margin: 2rem 0 3rem;
                  }
                  .wb-lead {
                    font-size: 1.15rem;
                    line-height: 1.7;
                    color: #1f2937;
                    max-width: 900px;
                  }
                  .wb-kicker {
                    display: inline-block;
                    font-size: .8rem;
                    letter-spacing: .06em;
                    text-transform: uppercase;
                    font-weight: 700;
                    color: #6b7280;
                    margin-bottom: .25rem;
                  }
                  .wb-hr {
                    height: 3px;
                    width: 72px;
                    background: linear-gradient(90deg, #111827, #6366f1 60%, #60a5fa);
                    border-radius: 999px;
                    margin: .75rem 0 1.25rem;
                  }
                  .wb-list {
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
                    gap: .75rem 1rem;
                    padding: 0;
                    margin: 1rem 0 0;
                    list-style: none;
                  }
                  .wb-list li {
                    background: #f9fafb;
                    border: 1px solid #e5e7eb;
                    border-radius: 12px;
                    padding: .75rem .9rem;
                    line-height: 1.5;
                  }

                  /* Definition cards */
                  #definition .wb-grid {
                    display: grid;
                    /* grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); */
                    grid-template-columns: repeat(3, minmax(0, 1fr));
                    gap: 1.25rem;
                    margin-top: 1rem;
                  }
                  .wb-card {
                    background: white;
                    border: 1px solid #e5e7eb;
                    border-radius: 14px;
                    padding: 1.1rem 1.1rem 1rem;
                    box-shadow: 0 1px 2px rgba(0,0,0,.04);
                    transition: transform .2s ease, box-shadow .2s ease, border-color .2s ease;
                  }
                  .wb-card:hover {
                    transform: translateY(-2px);
                    box-shadow: 0 8px 20px rgba(0,0,0,.06);
                    border-color: #d1d5db;
                  }
                  .wb-card h3 {
                    display: flex;
                    align-items: center;
                    gap: .5rem;
                    font-size: 1.05rem;
                    margin: 0 0 .4rem;
                  }
                  .wb-card p {
                    margin: .45rem 0;
                    color: #374151;
                    line-height: 1.6;
                  }
                  .wb-meta {
                    display: flex;
                    flex-wrap: wrap;
                    gap: .35rem .4rem;
                    margin-top: .5rem;
                  }
                  .wb-chip {
                    font-size: .75rem;
                    border: 1px solid #e5e7eb;
                    border-radius: 999px;
                    padding: .2rem .55rem;
                    background: #f3f4f6;
                    color: #374151;
                    white-space: nowrap;
                  }
                  .wb-cap {
                    font-size: .82rem;
                    color: #4b5563;
                    margin-top: .35rem;
                  }
                  .wb-anchor {
                    text-decoration: none;
                    color: inherit;
                  }
                </style>

                <div class="wb-section">
                  <div class="wb-kicker">Introduction</div>
                  <div class="wb-hr"></div>
                  <p class="wb-lead">
                    <strong>WorldBench</strong> benchmarks <em>3D/4D world models</em>—models that learn, predict, and simulate the
                    geometry and dynamics of real environments from multi-modal signals. We unify terminology, scope, and evaluation,
                    and organize the space into three complementary paradigms by representation: <strong>VideoGen</strong> (image/video-centric),
                    <strong>OccGen</strong> (occupancy-centric), and <strong>LiDARGen</strong> (point-cloud-centric).
                  </p>
                </div>
              </section>

              <!-- ---------- Definition (drop-in replacement for your current block) ---------- -->
              <section id="definition">
                <h2>Definition</h2>
                <div class="wb-hr" style="margin-bottom:1.25rem;"></div>

                <div class="wb-grid">
                  <!-- VideoGen -->
                  <article class="wb-card" id="videogen">
                    <h3>
                      <i class="fas fa-film" aria-hidden="true"></i>
                      <a class="wb-anchor" href="#videogen">VideoGen</a>
                    </h3>
                    <p>
                      <strong>What it is.</strong> An <em>image/video–centric</em> world model that generates or predicts
                      photorealistic, temporally consistent frames (single- or multi-view) conditioned on past context and optional controls.
                    </p>
                    <p class="wb-cap"><strong>Typical inputs</strong>: past frames, poses/camera intrinsics-extrinsics, text/route/action hints.</p>
                    <div class="wb-meta" aria-label="VideoGen tags">
                      <span class="wb-chip">Video synthesis</span>
                      <span class="wb-chip">Future prediction</span>
                      <span class="wb-chip">What-if simulation</span>
                      <span class="wb-chip">Multi-view</span>
                    </div>
                  </article>

                  <!-- OccGen -->
                  <article class="wb-card" id="occgen">
                    <h3>
                      <i class="fas fa-cubes" aria-hidden="true"></i>
                      <a class="wb-anchor" href="#occgen">OccGen</a>
                    </h3>
                    <p>
                      <strong>What it is.</strong> An <em>occupancy-centric</em> world model that represents scenes as 3D/4D occupancy
                      fields (e.g., voxel grids with semantics), enabling geometry-aware perception, forecasting, and simulation.
                    </p>
                    <p class="wb-cap"><strong>Typical inputs</strong>: multi-sensor cues (RGB, depth, events, LiDAR), ego motion, maps.</p>
                    <div class="wb-meta" aria-label="OccGen tags">
                      <span class="wb-chip">3D reconstruction</span>
                      <span class="wb-chip">Occupancy forecasting</span>
                      <span class="wb-chip">Autoregressive simulation</span>
                      <span class="wb-chip">Semantic voxels</span>
                    </div>
                  </article>

                  <!-- LiDARGen -->
                  <article class="wb-card" id="lidargen">
                    <h3>
                      <i class="fas fa-braille" aria-hidden="true"></i>
                      <a class="wb-anchor" href="#lidargen">LiDARGen</a>
                    </h3>
                    <p>
                      <strong>What it is.</strong> A <em>point-cloud–centric</em> world model that learns high-fidelity geometry and
                      dynamics directly from LiDAR sweeps, suitable for robust 3D understanding, data generation, and sensor-faithful simulation.
                    </p>
                    <p class="wb-cap"><strong>Typical inputs</strong>: past LiDAR sweeps, ego trajectory, calibration; optional scene priors.</p>
                    <div class="wb-meta" aria-label="LiDARGen tags">
                      <span class="wb-chip">Point-cloud synthesis</span>
                      <span class="wb-chip">Future sweeps</span>
                      <span class="wb-chip">Trajectory-conditioned</span>
                      <span class="wb-chip">Physics-aware</span>
                    </div>
                  </article>
                </div>
              </section>


              <style>
              /* 过渡段落略小一号 */
              #definition .wb-bridge {
                font-size: 1rem;
                line-height: 1.7;
                color: #374151;
                margin: 1rem 0 .75rem;
              }
              /* 4列卡片栅格（桌面四列 / 平板两到三列 / 手机一列） */
              #definition .wb-grid-4 {
                display: grid;
                grid-template-columns: repeat(4, minmax(0, 1fr));
                gap: 1.25rem;
                margin-top: .5rem;
              }
              @media (max-width: 1200px) {
                #definition .wb-grid-4 { grid-template-columns: repeat(3, minmax(0, 1fr)); }
              }
              @media (max-width: 1024px) {
                #definition .wb-grid-4 { grid-template-columns: repeat(2, minmax(0, 1fr)); }
              }
              @media (max-width: 640px) {
                #definition .wb-grid-4 { grid-template-columns: 1fr; }
              }
              /* 辅助：更紧凑的卡片标题与说明 */
              #definition .wb-card h4 {
                display: flex; align-items: center; gap: .5rem;
                font-size: 1rem; margin: 0 0 .35rem;
              }
              #definition .wb-card p.small {
                margin: .35rem 0; color: #4b5563; line-height: 1.6;
              }
            </style>

            <!-- 过渡衔接：放在三张 paradigms 卡片 .wb-grid 后 -->
            <p class="wb-bridge">
              Beyond the three paradigms above, we further distill <strong>four recurring model types</strong> that appear across
              VideoGen, OccGen, and LiDARGen.
            </p>

            <div class="wb-grid-4">
              <!-- 1) Data Engines -->
              <article class="wb-card" id="model-data-engines">
                <h4>
                  <i class="fas fa-database" aria-hidden="true"></i>
                  <a class="wb-anchor" href="#def-data-engines">Data Engines</a>
                </h4>
                <p class="small">
                  Systems that curate, augment, and generate multi-modal training data
                  (e.g., domain adaptation, controllable synthesis) to reduce distribution gaps and supervision cost.
                </p>
                <div class="wb-meta" aria-label="tags">
                  <span class="wb-chip">Curation</span>
                  <span class="wb-chip">Augmentation</span>
                  <span class="wb-chip">Synthetic data</span>
                </div>
              </article>

              <!-- 2) Scene Representors -->
              <article class="wb-card" id="model-scene-representors">
                <h4>
                  <i class="fas fa-cube" aria-hidden="true"></i>
                  <a class="wb-anchor" href="#def-scene-representors">Scene Representors</a>
                </h4>
                <p class="small">
                  Models that build consistent 3D/4D scene states (e.g., occupancy, meshes, radiance/feature fields) with semantics
                  and dynamics, serving as geometry-aware cores for perception and simulation.
                </p>
                <div class="wb-meta" aria-label="tags">
                  <span class="wb-chip">Reconstruction</span>
                  <span class="wb-chip">Semantic 3D</span>
                  <span class="wb-chip">Temporal state</span>
                </div>
              </article>

              <!-- 3) Forecasters & Planners -->
              <article class="wb-card" id="model-forecasters-planners">
                <h4>
                  <i class="fas fa-route" aria-hidden="true"></i>
                  <a class="wb-anchor" href="#def-forecasters-planners">Forecasters &amp; Planners</a>
                </h4>
                <p class="small">
                  Models that predict future world evolution under agent dynamics and constraints, supporting trajectory-aware
                  forecasting and planning-oriented evaluation.
                </p>
                <div class="wb-meta" aria-label="tags">
                  <span class="wb-chip">Future prediction</span>
                  <span class="wb-chip">Agent dynamics</span>
                  <span class="wb-chip">Open-loop metrics</span>
                </div>
              </article>

              <!-- 4) Autoregressive Simulators -->
              <article class="wb-card" id="model-autoregressive-simulators">
                <h4>
                  <i class="fas fa-play-circle" aria-hidden="true"></i>
                  <a class="wb-anchor" href="#def-autoregressive-simulators">Autoregressive Simulators</a>
                </h4>
                <p class="small">
                  Step-wise simulators that roll out world states conditioned on actions, maps, or priors, enabling counterfactual
                  and what-if analysis across modalities (video / occupancy / LiDAR).
                </p>
                <div class="wb-meta" aria-label="tags">
                  <span class="wb-chip">AR rollout</span>
                  <span class="wb-chip">What-if</span>
                  <span class="wb-chip">Multi-modal</span>
                </div>
              </article>
            </div>

            <section id="taxonomy">
                <h2>Taxonomy</h2>
                    <!-- Simple rounded image card -->
                      <!-- Simple rounded image card, constrained to body text width -->
                    <style>
                      #taxonomy .wb-simple-card{
                        width: min(100%, 72ch);           /* 改这里：72ch≈正文一行字符宽度 */
                        margin: 1rem auto 1.25rem;        /* 居中并与上下内容留白 */
                        border-radius: 16px;
                        overflow: hidden;
                        box-shadow: 0 8px 24px rgba(0,0,0,.08);
                        /* border: 1px solid #e5e7eb; */
                        background: #fff;
                      }
                      #taxonomy .wb-simple-card img{
                        display: block;
                        width: 100%;
                        height: auto;
                      }
                      @media (prefers-color-scheme: dark){
                        #taxonomy .wb-simple-card{
                          background: #0b1220;
                          border-color: #334155;
                          box-shadow: 0 8px 24px rgba(0,0,0,.35);
                        }
                      }
                    </style>

                    <figure class="wb-simple-card" aria-label="Taxonomy overview figure">
                      <img src="figures/tree.png" alt="WorldBench taxonomy tree">
                    </figure>


                    <!-- Then keep your existing iframes -->
                    <iframe
                      src="src/videogen.html"
                      style="width:100%; border:0; min-height:680px;"
                      loading="lazy"
                    ></iframe>

                    <!-- <iframe
                        src="src/conditions.html"
                        style="width:100%; border:0; min-height:680px;"
                        loading="lazy"
                    ></iframe>
                    <p> </p> -->
                    <!-- <iframe
                        src="src/videogen.html"
                        style="width:100%; border:0; min-height:680px;"
                        loading="lazy"
                    ></iframe>
                    <p> </p> -->
                    <iframe
                        src="src/occgen.html"
                        style="width:100%; border:0; min-height:680px;"
                        loading="lazy"
                    ></iframe>
                    <p> </p>
                    <iframe
                        src="src/lidargen.html"
                        style="width:100%; border:0; min-height:680px;"
                        loading="lazy"
                    ></iframe>
            </section>

            <section id="eamples">
                <h2>Examples</h2>

                <section class="videogen">
                    <h3>VideoGen</h3>
                  
                    <!-- 主播放器 -->
                    <div class="video-hero">
                      <video id="heroVideo" autoplay loop muted playsinline>
                        <source id="heroSource" src="videos/gt_0.mp4" type="video/mp4" />
                      </video>
                      <div class="video-badge" id="heroLabel">Reference</div>
                    </div>
                  
                    <!-- 缩略图/方法切换栏 -->
                    <div class="thumb-bar" role="tablist" aria-label="Video methods">
                      <!-- 每个按钮都带 data-*，点击即可切换 -->
                      <button class="thumb is-active" role="tab" aria-selected="true"
                              data-src="videos/gt_0.mp4" data-label="Reference">
                        <img src="thumbnails/gt_0.jpg" alt="Reference thumbnail" />
                        <span>Reference</span>
                      </button>
                  
                      <button class="thumb" role="tab" aria-selected="false"
                              data-src="videos/magicdrive_0.mp4" data-label="MagicDrive">
                        <img src="thumbnails/magicdrive_0.jpg" alt="MagicDrive thumbnail" />
                        <span>MagicDrive</span>
                      </button>
                  
                      <button class="thumb" role="tab" aria-selected="false"
                              data-src="videos/drivedreamer2.mp4" data-label="Drivedreamer2">
                        <img src="thumbnails/drivedreamer2.jpg" alt="Drivedreamer2 thumbnail" />
                        <span>Drivedreamer2</span>
                      </button>
                  
                      <button class="thumb" role="tab" aria-selected="false"
                              data-src="videos/dreamforge.mp4" data-label="Dreamforge">
                        <img src="thumbnails/dreamforge.jpg" alt="Dreamforge thumbnail" />
                        <span>Dreamforge</span>
                      </button>
                  
                      <button class="thumb" role="tab" aria-selected="false"
                              data-src="videos/opendwm.mp4" data-label="OpenDWM">
                        <img src="thumbnails/opendwm.jpg" alt="OpenDWM thumbnail" />
                        <span>OpenDWM</span>
                      </button>
                    </div>
                  </section>
                  
                  
                <h3>OccGen</h3>
                <p>...</p>

                <h3>LiDARGen</h3>
                <!-- LiDARGen -->
                <details class="lazy-load" id="lidargen">
                <summary>LiDARGen</summary>

                <div class="grid-2x2">
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/lidargen/0.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/lidargen/1.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/lidargen/2.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/lidargen/3.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                  </div>
                </details>

                <!-- LiDM -->
                <details class="lazy-load" id="lidargen">
                <summary>LiDM</summary>
                <div class="grid-2x2">
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/lidm/0.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/lidm/1.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/lidm/2.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/lidm/3.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                  </div>
                </details>

                <!-- R2DM -->
                <details class="lazy-load" id="lidargen">
                <summary>R2DM</summary>
                <div class="grid-2x2">
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/r2dm/0.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/r2dm/1.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/r2dm/2.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/r2dm/3.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                  </div>
                </details>

                <!-- UniScene -->
                <details class="lazy-load" id="lidargen">
                <summary>UniScene</summary>
                <div class="grid-2x2">
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/uniscene/0.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/uniscene/1.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/uniscene/2.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                
                    <iframe
                      class="viseriframe"
                      src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/uniscene/3.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                      loading="lazy" allowfullscreen>
                    </iframe>
                  </div>
                </details>

                <!-- OpenDWM -->
                <details class="lazy-load" id="lidargen">
                    <summary>OpenPWM</summary>
                    <div class="grid-2x2">
                        <iframe
                          class="viseriframe"
                          src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/opendwm/0.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                          loading="lazy" allowfullscreen>
                        </iframe>
                    
                        <iframe
                          class="viseriframe"
                          src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/opendwm/1.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                          loading="lazy" allowfullscreen>
                        </iframe>
                    
                        <iframe
                          class="viseriframe"
                          src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/opendwm/2.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                          loading="lazy" allowfullscreen>
                        </iframe>
                    
                        <iframe
                          class="viseriframe"
                          src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/opendwm/3.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                          loading="lazy" allowfullscreen>
                        </iframe>
                      </div>
                    </details>

                <!-- LiDARCrafter -->
                <details class="lazy-load" id="lidargen">
                    <summary>LiDARCrafter</summary>
                    <div class="grid-2x2">
                        <iframe
                          class="viseriframe"
                          src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/lidarcrafter/0.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                          loading="lazy" allowfullscreen>
                        </iframe>
                    
                        <iframe
                          class="viseriframe"
                          src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/lidarcrafter/1.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                          loading="lazy" allowfullscreen>
                        </iframe>
                    
                        <iframe
                          class="viseriframe"
                          src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/lidarcrafter/2.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                          loading="lazy" allowfullscreen>
                        </iframe>
                    
                        <iframe
                          class="viseriframe"
                          src="https://worldbench.github.io/viser-client/?playbackPath=https://worldbench.github.io/lidar_visers/lidarcrafter/3.viser&initialCameraPosition=10.885,9.746,4.995&initialCameraLookAt=2.343,1.204,-3.547&initialCameraUp=-0.000,-0.000,1.000"
                          loading="lazy" allowfullscreen>
                        </iframe>
                      </div>
                    </details>
                <!-- <p>...</p> -->

            </section>

            <section id="definition">
                <h2>Projects</h2>
            </section>
            <section id="video-papers" class="papers-section" data-section-key="video" data-initial="3">
                <h2>Video Generation</h2>
                <div class="bar" aria-live="polite">
                  <div class="tabs" role="tablist" aria-label="Video Generation Categories">
                    <button class="tab-btn tab-1" role="tab" aria-selected="true" aria-controls="video-panel-a" id="video-tab-a">Data Engines</button>
                    <button class="tab-btn tab-2" role="tab" aria-selected="false" aria-controls="video-panel-b" id="video-tab-b">Action Interpreters</button>
                    <button class="tab-btn tab-3" role="tab" aria-selected="false" aria-controls="video-panel-c" id="video-tab-c">Neural Simulators</button>
                    <button class="tab-btn tab-4" role="tab" aria-selected="false" aria-controls="video-panel-d" id="video-tab-d">Scene Reconstructors</button>
  
                </div>
                  <div class="search" title="输入关键词过滤（标题、作者、出处）">
                    🔎 <input class="paper-search" type="search" placeholder="Search: Title / Author / Source" />
                  </div>
                  <span class="count">0 items</span>
                </div>
                <section id="video-panel-a" class="panel" role="tabpanel" aria-labelledby="video-tab-a" aria-hidden="false">
                  <div class="list"></div>
                </section>
                <section id="video-panel-b" class="panel" role="tabpanel" aria-labelledby="video-tab-b" aria-hidden="true">
                  <div class="list"></div>
                </section>
                <section id="video-panel-c" class="panel" role="tabpanel" aria-labelledby="video-tab-c" aria-hidden="true">
                  <div class="list"></div>
                </section>
                <section id="video-panel-d" class="panel" role="tabpanel" aria-labelledby="video-tab-d" aria-hidden="true">
                  <div class="list"></div>
                </section>
              </section>

            <section id="occupancy-papers" class="papers-section" data-section-key="occupancy">
                <h2>Occupancy Generation</h2>
                <div class="bar" aria-live="polite">
                  <div class="tabs" role="tablist" aria-label="Occupancy Generation Categories">
                    <button class="tab-btn tab-1" role="tab" aria-selected="true" aria-controls="occupancy-panel-a" id="occupancy-tab-a">Scene Representors</button>
                    <button class="tab-btn tab-2" role="tab" aria-selected="false" aria-controls="occupancy-panel-b" id="occupancy-tab-b">Occupancy Forecasters</button>
                    <button class="tab-btn tab-3" role="tab" aria-selected="false" aria-controls="occupancy-panel-c" id="occupancy-tab-c">Autoregressive Simulators</button>
                    <!-- <button class="tab-btn tab-4" role="tab" aria-selected="false" aria-controls="video-panel-d" id="video-tab-d">Scene Reconstructors</button>   -->
                </div>
                  <div class="search" title="输入关键词过滤（标题、作者、出处）">
                    🔎 <input class="paper-search" type="search" placeholder="Search: Title / Author / Source" />
                  </div>
                  <span class="count">0 items</span>
                </div>
                <section id="occupancy-panel-a" class="panel" role="tabpanel" aria-labelledby="occupancy-tab-a" aria-hidden="false">
                  <div class="list"></div>
                </section>
                <section id="occupancy-panel-b" class="panel" role="tabpanel" aria-labelledby="occupancy-tab-b" aria-hidden="true">
                  <div class="list"></div>
                </section>
                <section id="occupancy-panel-c" class="panel" role="tabpanel" aria-labelledby="occupancy-tab-c" aria-hidden="true">
                  <div class="list"></div>
                </section>
                <section id="video-panel-d" class="panel" role="tabpanel" aria-labelledby="video-tab-d" aria-hidden="true">
                    <div class="list"></div>
                </section>
              </section>

              <section id="lidar-papers" class="papers-section" data-section-key="lidar">
                <h2>LiDAR Generation</h2>
                <div class="bar" aria-live="polite">
                  <div class="tabs" role="tablist" aria-label="LiDAR Generation Categories">
                    <button class="tab-btn tab-1" role="tab" aria-selected="true" aria-controls="lidar-panel-a" id="lidar-tab-a">Data Engines</button>
                    <button class="tab-btn tab-2" role="tab" aria-selected="false" aria-controls="lidar-panel-b" id="lidar-tab-b">Action Interpreters</button>
                    <button class="tab-btn tab-3" role="tab" aria-selected="false" aria-controls="lidar-panel-c" id="lidar-tab-c">Autoregressive Simulators</button>
                  </div>
                  <div class="search" title="输入关键词过滤（标题、作者、出处）">
                    🔎 <input class="paper-search" type="search" placeholder="Search: Title / Author / Source" />
                  </div>
                  <span class="count">0 items</span>
                </div>
                <section id="lidar-panel-a" class="panel" role="tabpanel" aria-labelledby="lidar-tab-a" aria-hidden="false">
                  <div class="list"></div>
                </section>
                <section id="lidar-panel-b" class="panel" role="tabpanel" aria-labelledby="lidar-tab-b" aria-hidden="true">
                  <div class="list"></div>
                </section>
                <section id="lidar-panel-c" class="panel" role="tabpanel" aria-labelledby="lidar-tab-c" aria-hidden="true">
                  <div class="list"></div>
                </section>
                <section id="video-panel-d" class="panel" role="tabpanel" aria-labelledby="video-tab-d" aria-hidden="true">
                    <div class="list"></div>
                </section>
              </section>

            <br><br>
            
            <d-appendix>
                <h3>BibTeX</h3>
                <p class="bibtex">
                    @article{kong2025worldbench,<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;title={WorldBench: Benchmarking 3D and 4D World Models in the Real World},<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;author={WorldBench Team},<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2508.xxxxx},<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;year={2025},<br>
                    }
                </p>

                <d-footnote-list></d-footnote-list>
                <d-citation-list></d-citation-list>
            </d-appendix>
              
                <d-footnote-list></d-footnote-list>
                <d-citation-list></d-citation-list>
            </d-appendix>

        </d-article>

        <script src="contents_bar.js"></script>

        <!-- 统一的数据与初始化脚本 -->
        <script>

          const paperStore = {
            video: {
              A: [
              { title:"BEVControl: Accurately Controlling Street-View Elements with Multi-Perspective Consistency via BEV Sketch Layout", authors:"", venue:"arXiv", year:2023, paper:"https://arxiv.org/abs/2308.01661", code:"#", project:"#"},
                { title:"Street-View Image Generation from a Bird's-Eye View Layout", authors:"", venue:"RA-L", year:2024, paper:"https://arxiv.org/abs/2301.04634", code:"https://github.com/alexanderswerdlow/BEVGen", project:"https://metadriverse.github.io/bevgen/"},
                { title:"MagicDrive: Street View Generation with Diverse 3D Geometry Control", authors:"", venue:"ICLR", year:2024, paper:"https://arxiv.org/abs/2310.02601", code:"https://github.com/cure-lab/MagicDrive", project:"https://gaoruiyuan.com/magicdrive/"},
                { title:"Panacea: Panoramic and Controllable Video Generation for Autonomous Driving", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2311.16813", code:"https://github.com/wenyuqing/panacea", project:"https://panacea-ad.github.io/"},
                { title:"DrivingDiffusion: Layout-Guided Multi-View Driving Scene Video Generation with Latent Diffusion Model", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2310.07771", code:"https://github.com/shalfun/DrivingDiffusion", project:"https://drivingdiffusion.github.io/"},
                { title:"WoVoGen: World Volume-Aware Diffusion for Controllable Multi-Camera Driving Scene Generation", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2312.02934", code:"https://github.com/fudan-zvg/WoVoGen", project:"#"},
                { title:"Unleashing Generalization of End-to-End Autonomous Driving with Controllable Long Video Generation (Delphi)", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2406.01349", code:"https://github.com/westlake-autolab/Delphi", project:"https://westlake-autolab.github.io/delphi.github.io/"},
                { title:"SimGen: Simulator-conditioned Driving Scene Generation", authors:"", venue:"NeurIPS", year:2024, paper:"https://arxiv.org/abs/2406.09386", code:"https://github.com/metadriverse/SimGen", project:"https://metadriverse.github.io/simgen/"},
                { title:"BEVWorld: A Multimodal World Simulator for Autonomous Driving via Scene-Level BEV Latents", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2407.05679", code:"#", project:"#"},
                { title:"Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2408.07605", code:"#", project:"https://panacea-ad.github.io/"},
                { title:"DiVE: DiT-Based Video Generation with Enhanced Control", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2409.01595", code:"https://github.com/LiAutoAD/DIVE", project:"https://liautoad.github.io/DIVE/"},
                { title:"SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2410.00337", code:"https://github.com/EnVision-Research/SyntheOcc", project:"https://len-li.github.io/syntheocc-web/"},
                { title:"HoloDrive: Holistic 2D-3D Multi-Modal Street Scene Generation for Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.01407", code:"#", project:"#"},
                { title:"Seeing Beyond Views: Multi-View Driving Scene Video Generation with Holistic Attention (CogDriving)", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.03520", code:"#", project:"https://luhannan.github.io/CogDrivingPage/"},
                { title:"UniMLVG: Unified Framework for Multi-View Long Video Generation with Comprehensive Control Capabilities for Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.04842", code:"https://github.com/SenseTime-FVG/OpenDWM", project:"#"},
                { title:"Physical Informed Driving World Model (DrivePhysica)", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.08410", code:"#", project:"https://metadrivescape.github.io/papers_project/DrivePhysica/page.html"},
                { title:"DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation", authors:"", venue:"AAAI", year:2025, paper:"https://arxiv.org/abs/2403.06845", code:"https://github.com/f1yfisher/DriveDreamer2", project:"https://drivedreamer2.github.io/"},
                { title:"SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject Control", authors:"", venue:"AAAI", year:2025, paper:"https://arxiv.org/abs/2403.19438", code:"#", project:"https://subjectdrive.github.io/"},
                { title:"Glad: A Streaming Scene Generator for Autonomous Driving", authors:"", venue:"ICLR", year:2025, paper:"https://arxiv.org/abs/2503.00045", code:"https://github.com/xb534/Glad", project:"#"},
                { title:"DualDiff: Dual-Branch Diffusion Model for Autonomous Driving with Semantic Fusion", authors:"", venue:"ICRA", year:2025, paper:"https://arxiv.org/abs/2505.01857", code:"https://github.com/yangzhaojason/DualDiff", project:"#"},
                { title:"UniScene: Unified Occupancy-Centric Driving Scene Generation", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2412.05435", code:"https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation", project:"https://arlo0o.github.io/uniscene/"},
                { title:"DriveScape: Towards High-Resolution Controllable Multi-View Driving Video Generation", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2409.05463", code:"#", project:"https://metadrivescape.github.io/papers_project/drivescapev1/index.html"},
                { title:"PerLDiff: Controllable Street View Synthesis Using Perspective-Layout Diffusion Models", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2407.06109", code:"https://github.com/LabShuHangGU/PerlDiff", project:"https://perldiff.github.io/"},
                { title:"MagicDrive-V2: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2411.13807", code:"#", project:"https://gaoruiyuan.com/magicdrive-v2/"},
                { title:"Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.14492", code:"https://github.com/nvidia-cosmos/cosmos-transfer1", project:"https://research.nvidia.com/labs/dir/cosmos-transfer1/"},
                { title:"DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.03689", code:"https://github.com/yangzhaojason/DualDiff", project:"#"},
                { title:"CoGen: 3D Consistent Video Generation via Adaptive Conditioning for Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.22231", code:"#", project:"https://xiaomi-research.github.io/cogen/"},
                { title:"NoiseController: Towards Consistent Multi-View Video Generation via Noise Decomposition and Collaboration", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2504.18448", code:"#", project:"#"},
                { title:"STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.13138", code:"#", project:"#"}
              ],
              B: [
              { title:"GAIA-1: A Generative World Model for Autonomous Driving", authors:"", venue:"arXiv", year:2023, paper:"https://arxiv.org/abs/2309.17080", code:"#", project:"https://wayve.ai/thinking/scaling-gaia-1/" },
                { title:"ADriver-I: A General World Model for Autonomous Driving", authors:"", venue:"arXiv", year:2023, paper:"https://arxiv.org/abs/2311.13549", code:"#", project:"#"},
                { title:"Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving (Drive-WM)", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2311.17918", code:"https://github.com/BraveGroup/Drive-WM", project:"https://drive-wm.github.io/" },
                { title:"DriveDreamer: Towards Real-World-Driven World Models for Autonomous Driving", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2309.09777", code:"https://github.com/JeffWang987/DriveDreamer", project:"https://drivedreamer.github.io/" },
                { title:"GenAD: Generalized Predictive Model for Autonomous Driving", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2403.09630", code:"https://github.com/OpenDriveLab/DriveAGI", project:"#"},
                { title:"Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability", authors:"", venue:"NeurIPS", year:2024, paper:"https://arxiv.org/abs/2405.17398", code:"https://github.com/OpenDriveLab/Vista", project:"https://vista-demo.github.io/" },
                { title:"InfinityDrive: Breaking Time Limits in Driving World Models", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.01522", code:"#", project:"https://metadrivescape.github.io/papers_project/InfinityDrive/page.html" },
                { title:"DrivingGPT: Unifying Driving World Modeling and Planning with Multi-Modal Autoregressive Transformers", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.18607", code:"#", project:"https://rogerchern.github.io/DrivingGPT/" },
                { title:"DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.19505", code:"https://github.com/YvanYin/DrivingWorld", project:"https://huxiaotaostasy.github.io/DrivingWorld/index.html" },
                { title:"GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2412.11198", code:"https://github.com/vita-epfl/GEM", project:"https://vita-epfl.github.io/GEM.github.io/" },
                { title:"MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2502.11663", code:"https://github.com/SenseTime-FVG/OpenDWM", project:"#"},
                { title:"Epona: Autoregressive Diffusion World Model for Autonomous Driving", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2506.24113", code:"https://github.com/Kevin-thu/Epona", project:"https://kevin-thu.github.io/Epona/" },
                { title:"VaViM and VaVAM: Autonomous Driving through Video Generative Modeling", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2502.15672", code:"https://github.com/valeoai/VideoActionModel", project:"https://valeoai.github.io/vavim-vavam/" },
                { title:"MiLA: Multi-View Intensive-Fidelity Long-Term Video Generation World Model for Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.15875", code:"https://github.com/xiaomi-mlab/mila.github.io", project:"#"},
                { title:"GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.20523", code:"#", project:"https://wayve.ai/thinking/gaia-2" },
                { title:"DriVerse: Navigation World Model for Driving Simulation via Multimodal Trajectory Prompting and Motion Alignment", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2504.18576", code:"#", project:"#"},
                { title:"PosePilot: Steering Camera Pose for Generative World Models with Self-Supervised Depth", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.01729", code:"#", project:"#"},
                { title:"ProphetDWM: A Driving World Model for Rolling Out Future Actions and Videos", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.18650", code:"#", project:"#"},
                { title:"LongDWM: Cross-Granularity Distillation for Building A Long-Term Driving World Model", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.01546", code:"https://github.com/Wang-Xiaodong1899/Long-DWM", project:"https://wang-xiaodong1899.github.io/longdwm/" }
              ],
              C: [
              { title:"MagicDrive3D: Controllable 3D Generation for Any-View Rendering in Street Scenes", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2405.14475", code:"https://github.com/flymin/MagicDrive3D", project:"https://gaoruiyuan.com/magicdrive3d/" },
                { title:"DreamForge: Motion-Aware Autoregressive Video Generation for Multi-View Driving Scenes", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2409.04003", code:"https://github.com/PJLab-ADG/DriveArena", project:"https://pjlab-adg.github.io/DriveArena/dreamforge/" },
                { title:"Doe-1: Closed-Loop Autonomous Driving with Large World Model", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.09627", code:"https://github.com/wzzheng/Doe", project:"https://wzzheng.net/Doe/" },
                { title:"DrivingSphere: Building A High-Fidelity 4D World for Closed-Loop Simulation", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2411.11252", code:"https://github.com/yanty123/DrivingSphere", project:"https://yanty123.github.io/DrivingSphere/" },
                { title:"UMGen: Generating Multimodal Driving Scenes via Next-Scene Prediction", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2503.14945", code:"https://github.com/YanhaoWu/UMGen", project:"https://yanhaowu.github.io/UMGen/" },
                { title:"DriveArena: A Closed-Loop Generative Simulation Platform for Autonomous Driving", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2408.00415", code:"https://github.com/PJLab-ADG/DriveArena", project:"https://pjlab-adg.github.io/DriveArena/" },
                { title:"InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2412.03934", code:"https://github.com/nv-tlabs/InfiniCube", project:"https://research.nvidia.com/labs/toronto-ai/infinicube/" },
                { title:"DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2503.15208", code:"https://github.com/royalmelon0505/dist4d", project:"https://royalmelon0505.github.io/DiST-4D/" },
                { title:"UniFuture: A Unified Driving World Model for Future Generation and Perception", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.13587", code:"https://github.com/dk-liang/UniFuture", project:"https://dk-liang.github.io/UniFuture/" },
                { title:"Nexus: Decoupled Diffusion Sparks Adaptive Scene Generation", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2504.10485", code:"https://github.com/OpenDriveLab/Nexus", project:"https://opendrivelab.com/Nexus/" },
                { title:"Challenger: Affordable Adversarial Driving Video Generation", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.15880", code:"https://github.com/Pixtella/Challenger", project:"https://pixtella.github.io/Challenger/" },
                { title:"Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.09042", code:"https://github.com/nv-tlabs/Cosmos-Drive-Dreams", project:"https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams/" }
              ],
              D: [
              { title:"3D Gaussian Splatting for Real-Time Radiance Field Rendering", authors:"", venue:"TOG", year:2023, paper:"https://arxiv.org/abs/2401.01339", code:"https://github.com/graphdeco-inria/gaussian-splatting", project:"https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/" },
                { title:"Street Gaussians: Modeling Dynamic Urban Scenes with Gaussian Splatting", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2401.01339", code:"https://github.com/zju3dv/street_gaussians", project:"https://zju3dv.github.io/street_gaussians" },
                { title:"Dynamic 3D Gaussian Fields for Urban Areas (4DGF)", authors:"", venue:"NeurIPS", year:2024, paper:"https://arxiv.org/abs/2406.03175", code:"https://github.com/tobiasfshr/map4d", project:"https://tobiasfshr.github.io/pub/4dgf/" },
                { title:"SCube: Instant Large-Scale Scene Reconstruction using VoxSplats", authors:"", venue:"NeurIPS", year:2024, paper:"https://arxiv.org/abs/2410.20030", code:"https://github.com/nv-tlabs/SCube", project:"https://research.nvidia.com/labs/toronto-ai/scube/" },
                { title:"HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2403.12722", code:"https://github.com/hyzhou404/HUGS", project:"https://xdimlab.github.io/hugs_website/" },
                { title:"MagicDrive3D: Controllable 3D Generation for Any-View Rendering in Street Scenes", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2405.14475", code:"https://github.com/flymin/MagicDrive3D", project:"https://gaoruiyuan.com/magicdrive3d/" },
                { title:"S3Gaussian: Self-Supervised Street Gaussians for Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2405.20323", code:"https://github.com/nnanhuang/S3Gaussian/", project:"https://wzzheng.net/S3Gaussian/" },
                { title:"VDG: Vision-Only Dynamic Gaussian for Driving Simulation", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2406.18198", code:"https://github.com/lifuguan/VDG_official", project:"https://3d-aigc.github.io/VDG/" },
                { title:"UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2411.15355", code:"#", project:"#"},
                { title:"Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.05280", code:"https://github.com/wzzheng/Stag", project:"https://wzzheng.net/Stag/" },
                { title:"DrivingRecon: Large 4D Gaussian Reconstruction Model For Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.09043", code:"https://github.com/EnVision-Research/DriveRecon", project:"#"},
                { title:"OccScene: Semantic Occupancy-Based Cross-Task Mutual Learning for 3D Scene Generation", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.11183", code:"#", project:"#"},
                { title:"SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior", authors:"", venue:"WACV", year:2025, paper:"https://arxiv.org/abs/2403.20079", code:"#", project:"#"},
                { title:"OmniRe: Omni Urban Scene Reconstruction", authors:"", venue:"ICLR", year:2025, paper:"https://arxiv.org/abs/2408.16760", code:"https://github.com/ziyc/drivestudio", project:"https://ziyc.github.io/omnire/" },
                { title:"DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2410.13571", code:"https://github.com/GigaAI-research/DriveDreamer4D", project:"https://drivedreamer4d.github.io/" },
                { title:"DeSiRe-GS: 4D Street Gaussians for Static-Dynamic Decomposition and Surface Reconstruction for Urban Driving Scenes", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2411.11921", code:"https://github.com/chengweialan/DeSiRe-GS", project:"#"},
                { title:"SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2411.16816", code:"https://github.com/carlinds/splatad", project:"https://research.zenseact.com/publications/splatad/" },
                { title:"ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2411.19548", code:"https://github.com/GigaAI-research/ReconDreamer/", project:"https://recondreamer.github.io/" },
                { title:"FreeSim: Toward Free-Viewpoint Camera Simulation in Driving Scenes", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2412.03566", code:"#", project:"https://drive-sim.github.io/freesim/" },
                { title:"StreetCrafter: Street View Synthesis with Controllable Video Diffusion Models", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2412.13188", code:"https://github.com/zju3dv/street_crafter", project:"https://zju3dv.github.io/street_crafter/" },
                { title:"FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction and Rendering", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2502.21093", code:"#", project:"#"},
                { title:"S-NeRF++: Autonomous Driving Simulation via Neural Reconstruction and Generation", authors:"", venue:"TPAMI", year:2025, paper:"https://arxiv.org/abs/2402.02112", code:"#", project:"#"},
                { title:"DreamDrive: Generative 4D Scene Modeling from Street View Images", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2501.00601", code:"#", project:"https://pointscoder.github.io/DreamDrive/" },
                { title:"Uni-Gaussians: Unifying Camera and Lidar Simulation with Gaussians for Dynamic Driving Scenarios", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.08317", code:"#", project:"https://zikangyuan.github.io/UniGaussians/" },
                { title:"MuDG: Taming Multi-Modal Diffusion with Gaussian Splatting for Urban Scene Reconstruction", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.10604", code:"https://github.com/heiheishuang/MuDG", project:"https://heiheishuang.xyz/mudg/" },
                { title:"SceneCrafter: Unraveling the Effects of Synthetic Data on End-to-End Autonomous Driving Humanoid Robots", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.18108", code:"https://github.com/cancaries/SceneCrafter", project:"#"},
                { title:"ReconDreamer++: Harmonizing Generative and Reconstructive Models for Driving Scene Representation", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.18438", code:"https://github.com/GigaAI-research/ReconDreamer-Plus", project:"https://recondreamer-plus.github.io/" },
                { title:"RealEngine: Simulating Autonomous Driving in Realistic Context", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.16902", code:"https://github.com/fudan-zvg/RealEngine", project:"#"},
                { title:"GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.22421", code:"https://github.com/antonioo-c/GeoDrive", project:"#"},
                { title:"Pseudo-Simulation for Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.04218", code:"https://github.com/autonomousvision/navsim", project:"#"},
                { title:"Dreamland: Controllable World Creation with Simulator and Generative Models", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.08006", code:"#", project:"https://metadriverse.github.io/dreamland/" }
              ]
            },
            occupancy: {
              A: [
              { title:"Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data (SSD)", authors:"", venue:"arXiv", year:2023, paper:"https://arxiv.org/abs/2301.00527", code:"https://github.com/zoomin-lee/scene-scale-diffusion", project:"#"},
                { title:"SemCity: Semantic Scene Generation with Triplane Diffusion", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2403.07773", code:"https://github.com/zoomin-lee/SemCity", project:"https://sglab.kaist.ac.kr/SemCity/"},
                { title:"WoVoGen: World Volume-Aware Diffusion for Controllable Multi-Camera Driving Scene Generation", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2312.02934", code:"https://github.com/fudan-zvg/WoVoGen", project:"#"},
                { title:"Urban Scene Diffusion through Semantic Occupancy Map (UrbanDiff)", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2403.11697", code:"#", project:"https://metadriverse.github.io/urbandiff/"},
                { title:"DrivingSphere: Building A High-Fidelity 4D World for Closed-Loop Simulation", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2411.11252", code:"https://github.com/yanty123/DrivingSphere", project:"https://yanty123.github.io/DrivingSphere/"},
                { title:"UniScene: Unified Occupancy-Centric Driving Scene Generation", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2412.05435", code:"https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation", project:"https://arlo0o.github.io/uniscene/"},
                { title:"OccScene: Semantic Occupancy-Based Cross-Task Mutual Learning for 3D Scene Generation", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.11183", code:"#", project:"#"},
                { title:"InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2412.03934", code:"https://github.com/nv-tlabs/InfiniCube", project:"https://research.nvidia.com/labs/toronto-ai/infinicube/"},
                { title:"Controllable 3D Outdoor Scene Generation via Scene Graphs (Control-3D-Scene)", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2503.07152", code:"https://github.com/yuhengliu02/control-3d-scene", project:"https://yuheng.ink/project-page/control-3d-scene/"},
                { title:"X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.13558", code:"https://github.com/yuyang-cloud/X-Scene", project:"https://x-scene.github.io/"},
              ],
              B: [
              { title:"Emergent-Occ: Differentiable Raycasting for Self-supervised Occupancy Forecasting", authors:"", venue:"ECCV", year:2022, paper:"https://arxiv.org/abs/2210.01917", code:"https://github.com/tarashakhurana/emergent-occ-forecasting", project:"#"},
                { title:"FF4D: Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting", authors:"", venue:"CVPR", year:2023, paper:"https://arxiv.org/abs/2302.13130", code:"https://github.com/tarashakhurana/4d-occ-forecasting", project:"https://www.cs.cmu.edu/~tkhurana/ff4d/index.html"},
                { title:"UniWorld: Autonomous Driving Pre-Training via World Models", authors:"", venue:"arXiv", year:2023, paper:"https://arxiv.org/abs/2308.07234", code:"#", project:"#"},
                { title:"UniScene: Multi-Camera Unified Pre-Training via 3D Scene Reconstruction for Autonomous Driving", authors:"", venue:"arXiv", year:2023, paper:"https://arxiv.org/abs/2305.18829", code:"https://github.com/chaytonmin/UniScene", project:"#"},
                { title:"OccWorld: Learning A 3D Occupancy World Model for Autonomous Driving", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2311.16038", code:"https://github.com/wzzheng/OccWorld", project:"https://wzzheng.net/OccWorld/"},
                { title:"Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2311.17663", code:"https://github.com/haomo-ai/Cam4DOcc", project:"#"},
                { title:"DriveWorld: 4D Pre-Trained Scene Understanding via World Models", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2405.04390", code:"#", project:"#"},
                { title:"OccSora: 4D Occupancy Generation Models as World Simulators", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2405.20337", code:"https://github.com/wzzheng/OccSora", project:"https://wzzheng.net/OccSora/"},
                { title:"UnO: Unsupervised Occupancy Fields for Perception and Forecasting", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2406.08691", code:"#", project:"https://waabi.ai/uno/"},
                { title:"LOPR: Self-Supervised Multi-Future Occupancy Forecasting", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2407.21126", code:"#", project:"#"},
                { title:"FSF-Net: Enhance 4D Occupancy Forecasting with Coarse BEV Scene Flow", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2409.15841", code:"#", project:"#"},
                { title:"OccLLaMA: An Occupancy-Language-Action Generative World Model", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2409.03272", code:"#", project:"#"},
                { title:"DOME: Taming Diffusion Model into High-Fidelity Controllable Occupancy World Model", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2410.10429", code:"https://github.com/gusongen/DOME", project:"https://gusongen.github.io/DOME"},
                { title:"GaussianAD: Gaussian-Centric End-to-End Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.10371", code:"https://github.com/wzzheng/GaussianAD", project:"https://wzzheng.net/GaussianAD"},
                { title:"DFIT-OccWorld: Efficient Occupancy World Model via Decoupled Dynamic Flow", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.13772", code:"#", project:"#"},
                { title:"Drive-OccWorld: Vision-Centric 4D Occupancy Forecasting and Planning", authors:"", venue:"AAAI", year:2025, paper:"https://arxiv.org/abs/2408.14197", code:"https://github.com/yuyang-cloud/Drive-OccWorld", project:"https://drive-occworld.github.io/"},
                { title:"PreWorld: Semi-Supervised Vision-Centric 3D Occupancy World Model", authors:"", venue:"ICLR", year:2025, paper:"https://arxiv.org/abs/2502.07309", code:"https://github.com/getterupper/PreWorld", project:"#"},
                { title:"OccProphet: Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner", authors:"", venue:"ICLR", year:2025, paper:"https://arxiv.org/abs/2502.15180", code:"https://github.com/JLChen-C/OccProphet", project:"#"},
                { title:"RenderWorld: World Model with Self-Supervised 3D Label", authors:"", venue:"ICRA", year:2025, paper:"https://arxiv.org/abs/2409.11356", code:"#", project:"#"},
                { title:"Occ-LLM: Enhancing Driving with Occupancy-Based Large Language Models", authors:"", venue:"ICRA", year:2025, paper:"https://arxiv.org/abs/2502.06419", code:"#", project:"#"},
                { title:"EfficientOCF: Spatiotemporal Decoupling for Efficient Occupancy Forecasting", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2411.14169", code:"#", project:"#"},
                { title:"DIO: Decomposable Implicit 4D Occupancy-Flow World Model", authors:"", venue:"CVPR", year:2025, paper:"https://openaccess.thecvf.com/content/CVPR2025/papers/Diehl_DIO_Decomposable_Implicit_4D_Occupancy-Flow_World_Model_CVPR_2025_paper.pdf", code:"#", project:"#"},
                { title:"T³Former: Temporal Triplane Transformers as Occupancy World Models", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.07338", code:"#", project:"#"},
                { title:"UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2503.24381", code:"https://github.com/tasl-lab/UniOcc", project:"https://huggingface.co/datasets/tasl-lab/uniocc"},
                { title:"I²-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2507.09144", code:"https://github.com/lzzzzzm/II-World", project:"#"},
                { title:"COME: Adding Scene-Centric Forecasting Control to Occupancy World Model", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.13260", code:"https://github.com/synsin0/COME", project:"#"}
              ],
              C: [
              { title:"SemCity: Semantic Scene Generation with Triplane Diffusion", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2403.07773", code:"https://github.com/zoomin-lee/SemCity", project:"https://sglab.kaist.ac.kr/SemCity/"},
                { title:"XCube: Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2312.03806", code:"https://github.com/nv-tlabs/XCube", project:"https://research.nvidia.com/labs/toronto-ai/xcube/"},
                { title:"PDD: Pyramid Diffusion for Fine 3D Large Scene Generation", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2311.12085", code:"https://github.com/yuhengliu02/pyramid-discrete-diffusion", project:"https://yuheng.ink/project-page/pyramid-discrete-diffusion"},
                { title:"OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2405.20337", code:"https://github.com/wzzheng/OccSora", project:"https://wzzheng.net/OccSora/"},
                { title:"DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes", authors:"", venue:"ICLR", year:2025, paper:"https://arxiv.org/abs/2410.18084", code:"https://github.com/3DTopia/DynamicCity", project:"https://dynamic-city.github.io/"},
                { title:"DrivingSphere: Building A High-Fidelity 4D World for Closed-Loop Simulation", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2411.11252", code:"https://github.com/yanty123/DrivingSphere", project:"https://yanty123.github.io/DrivingSphere/"},
                { title:"InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2412.03934", code:"https://github.com/nv-tlabs/InfiniCube", project:"https://research.nvidia.com/labs/toronto-ai/infinicube/"},
                { title:"X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.13558", code:"https://github.com/yuyang-cloud/X-Scene", project:"https://x-scene.github.io/"},
                { title:"PrITTI: Primitive-Based Generation of Controllable and Editable 3D Semantic Scenes", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2506.19117", code:"https://github.com/avg-dev/PrITTI", project:"https://raniatze.github.io/pritti/"}

              ]
            },
            lidar: {
              A: [
              { title:"DUSty: Learning to Drop Points for LiDAR Scan Synthesis", authors:"", venue:"IROS", year:2021, paper:"https://arxiv.org/abs/2102.11952", code:"https://github.com/kazuto1011/dusty-gan", project:"https://kazuto1011.github.io/dusty-gan/" },
                { title:"LiDARGen: Learning to Generate Realistic LiDAR Point Clouds", authors:"", venue:"ECCV", year:2022, paper:"https://arxiv.org/abs/2209.03954", code:"https://github.com/vzyrianov/lidargen", project:"#"},
                { title:"DUSty v2: Generative Range Imaging for Learning Scene Priors of 3D LiDAR Data", authors:"", venue:"WACV", year:2023, paper:"https://arxiv.org/abs/2210.11750", code:"https://github.com/kazuto1011/dusty-gan-v2", project:"https://kazuto1011.github.io/dusty-gan-v2/"},
                { title:"UltraLiDAR: Learning Compact Representations for LiDAR Completion and Generation", authors:"", venue:"CVPR", year:2023, paper:"https://arxiv.org/abs/2311.01448", code:"#", project:"https://waabi.ai/ultralidar/"},
                { title:"Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion", authors:"", venue:"ICLR", year:2024, paper:"https://arxiv.org/abs/2311.01017", code:"#", project:"https://waabi.ai/research/copilot-4d"},
                { title:"R2DM: LiDAR Data Synthesis with Denoising Diffusion Probabilistic Models", authors:"", venue:"ICRA", year:2024, paper:"https://arxiv.org/abs/2309.09256", code:"https://github.com/kazuto1011/r2dm", project:"https://kazuto1011.github.io/r2dm"},
                { title:"ViDAR: Visual Point Cloud Forecasting enables Scalable Autonomous Driving", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2312.17655", code:"https://github.com/OpenDriveLab/ViDAR", project:"#"},
                { title:"LiDiff: Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2403.13470", code:"https://github.com/PRBonn/LiDiff", project:"#"},
                { title:"LiDM: Towards Realistic Scene Generation with LiDAR Diffusion Models", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2404.00815", code:"https://github.com/hancyran/LiDAR-Diffusion/tree/8416ddbbda881553088109a66badf71ff11999e0", project:"#"},
                { title:"RangeLDM: Fast Realistic LiDAR Point Cloud Generation", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2403.10094", code:"https://github.com/WoodwindHu/RangeLDM", project:"#"},
                { title:"Text2LiDAR: Text-Guided LiDAR Point Cloud Generation via Equirectangular Transformer", authors:"", venue:"ECCV", year:2024, paper:"https://arxiv.org/abs/2407.19628", code:"https://github.com/wuyang98/Text2LiDAR", project:"#"},
                { title:"LiDARGRIT: Taming Transformers for Realistic Lidar Point Cloud Generation", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2404.05505", code:"https://github.com/hamedhaghighi/LidarGRIT", project:"#"},
                { title:"BEVWorld: A Multimodal World Simulator for Autonomous Driving via Scene-Level BEV Latents", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2407.05679", code:"https://github.com/zympsyche/BevWorld", project:"#"},
                { title:"SDS: Simultaneous Diffusion Sampling for Conditional LiDAR Generation", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2410.11628", code:"#", project:"#"},
                { title:"DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion Probabilistic Models", authors:"", venue:"IROS", year:2025, paper:"https://arxiv.org/abs/2409.18092", code:"#", project:"#"},
                { title:"HoloDrive: Holistic 2D-3D Multi-Modal Street Scene Generation for Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.01407", code:"#", project:"#"},
                { title:"LOGen: Toward Lidar Object Generation by Point Diffusion", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.07385", code:"https://github.com/valeoai/LOGen", project:"https://nerminsamet.github.io/logen/"},
                { title:"OLiDM: Object-Aware LiDAR Diffusion Models for Autonomous Driving", authors:"", venue:"AAAI", year:2025, paper:"https://arxiv.org/abs/2412.17226", code:"https://github.com/yanty123/OLiDM", project:"https://yanty123.github.io/OLiDM"},
                { title:"X-Drive: Cross-Modality Consistent Multi-Sensor Data Synthesis for Driving Scenarios", authors:"", venue:"ICLR", year:2025, paper:"https://arxiv.org/abs/2411.01123", code:"https://github.com/yichen928/X-Drive", project:"#"},
                { title:"LidarDM: Generative LiDAR Simulation in a Generated World", authors:"", venue:"ICRA", year:2025, paper:"https://arxiv.org/abs/2404.02903", code:"https://github.com/vzyrianov/lidardm", project:"https://zyrianov.org/lidardm/"},
                { title:"LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in Real-World Scenes", authors:"", venue:"ICRA", year:2025, paper:"https://arxiv.org/abs/2412.00592", code:"https://github.com/HoAdrian/ICRA2025_lidar_edit", project:"https://sites.google.com/view/lidar-edit"},
                { title:"R2Flow: Fast LiDAR Data Generation with Rectified Flows", authors:"", venue:"ICRA", year:2025, paper:"https://arxiv.org/abs/2412.02241", code:"https://github.com/kazuto1011/r2flow", project:"https://kazuto1011.github.io/r2flow/"},
                { title:"WeatherGen: A Unified Diverse Weather Generator for LiDAR Point Clouds via Spider Mamba Diffusion", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2504.13561", code:"https://github.com/wuyang98/weathergen", project:"#"},
                { title:"LiDPM: Rethinking Point Diffusion for Lidar Scene Completion", authors:"", venue:"IV", year:2025, paper:"https://arxiv.org/abs/2504.17791", code:"https://github.com/astra-vision/LiDPM", project:"https://astra-vision.github.io/LiDPM/"},
                { title:"HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2501.14729", code:"https://github.com/LMD0311/HERMES", project:"https://lmd0311.github.io/HERMES/"},
                { title:"SuperPC: A Single Diffusion Model for Point Cloud Completion, Upsampling, Denoising, and Colorization", authors:"", venue:"CVPR", year:2025, paper:"https://arxiv.org/abs/2503.14558", code:"#", project:"https://sairlab.org/superpc/"},
                { title:"3DiSS: Towards Generating Realistic 3D Semantic Training Data for Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2503.21449", code:"https://github.com/PRBonn/3DiSS", project:"#"},
                { title:"Distill-DPO: Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2504.11447", code:"https://github.com/happyw1nd/DistillationDPO", project:"#"},
                { title:"DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.19239", code:"#", project:"#"},
                { title:"OpenDWM: Open Driving World Models", authors:"", venue:"arXiv", year:2025, paper:"https://github.com/SenseTime-FVG/OpenDWM", code:"https://github.com/SenseTime-FVG/OpenDWM", project:"#"},
                { title:"SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.22643", code:"#", project:"#"},
                { title:"La La LiDAR: Large-Scale Layout Generation from LiDAR Data", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2508.03691", code:"#", project:"#"},
                { title:"Veila: Panoramic LiDAR Generation from a Monocular RGB Image", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2508.03690", code:"#", project:"#"},
                { title:"LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2508.03692", code:"https://github.com/lidarcrafter/toolkit", project:"https://lidarcrafter.github.io/" }
  
              ],
              B: [
              { title:"Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion", authors:"", venue:"ICLR", year:2024, paper:"https://arxiv.org/abs/2311.01017", code:"#", project:"https://waabi.ai/research/copilot-4d"},
                { title:"Visual Point Cloud Forecasting enables Scalable Autonomous Driving (ViDAR)", authors:"", venue:"CVPR", year:2024, paper:"https://arxiv.org/abs/2312.17655", code:"https://github.com/OpenDriveLab/ViDAR", project:"#"},
                { title:"BEVWorld: A Multimodal World Simulator for Autonomous Driving via Scene-Level BEV Latents", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2407.05679", code:"https://github.com/zympsyche/BevWorld", project:"#"},
                { title:"HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation", authors:"", venue:"ICCV", year:2025, paper:"https://arxiv.org/abs/2501.14729", code:"https://github.com/LMD0311/HERMES", project:"https://lmd0311.github.io/HERMES/"},
                { title:"DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2505.19239", code:"#", project:"#"}
              ],
              C: [
              { title:"HoloDrive: Holistic 2D-3D Multi-Modal Street Scene Generation for Autonomous Driving", authors:"", venue:"arXiv", year:2024, paper:"https://arxiv.org/abs/2412.01407", code:"#", project:"#"},
                { title:"LidarDM: Generative LiDAR Simulation in a Generated World", authors:"", venue:"ICRA", year:2025, paper:"https://arxiv.org/abs/2404.02903", code:"https://github.com/vzyrianov/lidardm", project:"https://zyrianov.org/lidardm/"},
                { title:"OpenDWM: Open Driving World Models", authors:"", venue:"arXiv", year:2025, paper:"https://github.com/SenseTime-FVG/OpenDWM", code:"https://github.com/SenseTime-FVG/OpenDWM", project:"#"},
                { title:"LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences", authors:"", venue:"arXiv", year:2025, paper:"https://arxiv.org/abs/2508.03692", code:"https://github.com/lidarcrafter/toolkit", project:"https://lidarcrafter.github.io/" }
              ]
            }
          };
    
          (function initAllPapersSections(){
            document.querySelectorAll('.papers-section').forEach(section => initPapersSection(section));
          })();
    
          function initPapersSection(root){
            const sectionKey = root.getAttribute('data-section-key'); // video / occupancy / lidar
            const tabs = Array.from(root.querySelectorAll('.tab-btn'));
            const panels = {
              A: root.querySelector(`[id$="panel-a"]`),
              B: root.querySelector(`[id$="panel-b"]`),
              C: root.querySelector(`[id$="panel-c"]`),
              D: root.querySelector(`[id$="panel-d"]`),
            };
            const lists = {
              A: panels.A.querySelector('.list'),
              B: panels.B.querySelector('.list'),
              C: panels.C.querySelector('.list'),
              D: panels.D.querySelector('.list')
            };
            const countEl = root.querySelector('.count');
            const searchInput = root.querySelector('.paper-search');
    
            let activeKey = 'A';
            let keyword = '';
    
            function makeLink(label, href){
              const a = document.createElement('a');
              a.className = 'link';
              a.textContent = label;
              a.href = href || '#';
              a.target = '_blank';
              a.rel = 'noopener noreferrer';
              return a;
            }
            function makeCard(item, badgeIdx=1){
              const wrap = document.createElement('article');
              wrap.className = 'card';
    
              const dot = document.createElement('span');
              dot.className = 'badge' + (badgeIdx===2?' badge-2': (badgeIdx===3?' badge-3':(badgeIdx===4?' badge-4':'')));
              wrap.appendChild(dot);
    
              const box = document.createElement('div');
              const h3 = document.createElement('h3');
              h3.textContent = item.title || 'Untitled';
              const meta = document.createElement('div');
              meta.className = 'meta';
              meta.textContent = [
                item.authors || '',
                item.venue ? `${item.venue}${item.year?` · ${item.year}`:''}` : (item.year||'')
              ].filter(Boolean).join(' | ');
    
              const links = document.createElement('div');
              links.className = 'links';
              links.appendChild(makeLink('Paper', item.paper));
              links.appendChild(makeLink('Code', item.code));
              links.appendChild(makeLink('Project', item.project));
    
              box.appendChild(h3);
              box.appendChild(meta);
              box.appendChild(links);
              wrap.appendChild(box);
              return wrap;
            }
    
            const normalize = s => (s||'').toLowerCase();
            function match(item, kw){
              if(!kw) return true;
              const bag = [item.title, item.authors, item.venue, item.year]
                .map(x=>String(x??'')).join(' ').toLowerCase();
              return bag.includes(kw);
            }
    
            function renderList(subKey){
                const INITIAL = Number(root.dataset.initial || 3); // 默认 3
                const data = (paperStore[sectionKey] && paperStore[sectionKey][subKey]) || [];
                const items = data.filter(it=>match(it, keyword));

                const panelEl = panels[subKey];
                const listEl  = lists[subKey];

                let ctrlEl = panelEl.querySelector('.showmore-wrap');
                if(!ctrlEl){
                    ctrlEl = document.createElement('div');
                    ctrlEl.className = 'showmore-wrap';
                    panelEl.appendChild(ctrlEl);
                }

                if(panelEl.dataset.expanded === undefined){
                    panelEl.dataset.expanded = "false";
                }
                const expanded = panelEl.dataset.expanded === "true";

                const shown = expanded ? items : items.slice(0, INITIAL);

                listEl.innerHTML = '';
                if(items.length===0){
                    const empty = document.createElement('div');
                    empty.className = 'empty';
                    empty.textContent = 'No data or no match.';
                    listEl.appendChild(empty);
                    ctrlEl.innerHTML = '';
                    countEl.textContent = `0 items`;
                    return;
                }
                shown.forEach(it => listEl.appendChild(makeCard(
                    it, subKey==='A'?1: subKey==='B'?2: subKey==='C'?3:4
                )));

                countEl.textContent = `${shown.length}/${items.length} items`;

                if(items.length > INITIAL){
                    ctrlEl.innerHTML = `
                    <button class="showmore-btn" type="button">
                        ${expanded ? "Collapse" : `Expand (${items.length - INITIAL})`}
                    </button>`;
                    const btn = ctrlEl.querySelector('button');
                    btn.onclick = () => {
                    panelEl.dataset.expanded = expanded ? "false" : "true";
                    renderList(subKey);
                    };
                }else{
                    ctrlEl.innerHTML = '';
                }
                }

    
            function switchTo(subKey){
              activeKey = subKey;
              panels[activeKey].dataset.expanded = "false";
              tabs.forEach(btn=>{
                const isSelected = btn.id.endsWith(subKey.toLowerCase());
                btn.setAttribute('aria-selected', String(isSelected));
              });
              Object.entries(panels).forEach(([k,el])=>{
                el.setAttribute('aria-hidden', String(k!==subKey));
              });
              renderList(subKey);
            }
    
            // events
            tabs.forEach(btn=>{
              btn.addEventListener('click', ()=>{
                const subKey = btn.id.split('-').pop().toUpperCase(); // a/b/c
                switchTo(subKey);
              });
            });
            root.addEventListener('keydown', (e)=>{
            if(e.key==='ArrowRight' || e.key==='ArrowLeft'){
                e.preventDefault();
                const order = ['A','B','C','D'].filter(k => panels[k]); // 只保留存在的面板
                const idx = order.indexOf(activeKey);
                const len = order.length;
                const next = e.key==='ArrowRight' ? (idx+1)%len : (idx-1+len)%len;
                switchTo(order[next]);
                root.querySelector(`[id$="tab-${order[next].toLowerCase()}"]`)?.focus();
            }
            });
            searchInput.addEventListener('input', ()=>{
              keyword = normalize(searchInput.value);
              renderList(activeKey);
            });
            searchInput.addEventListener('search', ()=>{
              keyword = normalize(searchInput.value);
              renderList(activeKey);
            });
    
            // init
            switchTo('A');
          }
        </script>

        <script>
            (function () {
            const heroVideo  = document.getElementById('heroVideo');
            const heroSource = document.getElementById('heroSource');
            const heroLabel  = document.getElementById('heroLabel');
            const thumbs     = Array.from(document.querySelectorAll('.thumb-bar .thumb'));
        
            function setActive(btn) {
                thumbs.forEach(b => {
                b.classList.toggle('is-active', b === btn);
                b.setAttribute('aria-selected', b === btn ? 'true' : 'false');
                });
            }
        
            function switchTo(btn) {
                const src   = btn.getAttribute('data-src');
                const label = btn.getAttribute('data-label') || 'Video';
                if (!src) return;
        
                // 切换源：重载后保持自动播放/循环/静音
                heroSource.src = src;
                heroVideo.load();
                heroVideo.play().catch(()=>{}); // 某些浏览器需要 catch
                heroLabel.textContent = label;
                setActive(btn);
            }
        
            // 点击切换
            thumbs.forEach(btn => {
                btn.addEventListener('click', () => switchTo(btn));
            });
        
            // 键盘左右切换（聚焦整个组件时也生效）
            document.addEventListener('keydown', (e) => {
                if (e.key !== 'ArrowLeft' && e.key !== 'ArrowRight') return;
                const idx = thumbs.findIndex(b => b.classList.contains('is-active'));
                if (idx === -1) return;
                const nextIdx = e.key === 'ArrowRight'
                ? (idx + 1) % thumbs.length
                : (idx - 1 + thumbs.length) % thumbs.length;
                switchTo(thumbs[nextIdx]);
                thumbs[nextIdx].scrollIntoView({ behavior: 'smooth', inline: 'center', block: 'nearest' });
            });
            })();
        </script>
  

    </body>
</html>
